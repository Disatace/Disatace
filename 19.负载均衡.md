# 负载均衡

## nginx负载均衡

### 介绍

负载均衡（Server Load Balancer）是将访问流量根据转发策略分发到后端多台 ECS 的流量分发控制服 务。负载均衡可 以通过流量分发扩展应用系统对外的服务能力，通过消除单点故障提升应用系统的可用 性。 负载均衡主要有如下几个功能点：

- 负载均衡服务通过设置虚拟ip，将位于同一地域（Region）的多台云服务器（Elastic Compute Service，**简称ECS**）资源虚拟成一个高性能、高可用的应用服务池；根据应用指定的方 式，将来自客户端的网络请 求分发到云服务器池中。 
- 负载均衡服务会检查云服务器池中ECS的健康状态，自动隔离异常状态的ECS，从而解决了单台ECS 的单点问题，同 时提高了应用的整体服务能力。在标准的负载均衡功能之外，负载均衡服务还具备 TCP与HTTP抗DDoS攻击的特 性，增强了应用服务器的防护能力。 
- 负载均衡服务是ECS面向多机方案的一个配套服务，需要同ECS结合使用。

## ngx_http_upstream_module模块

相关参数：

- upstream：定义后端服务器组，会引入一个新的上下文 
- server：在upstream上下文中server成员，以及相关的参数 
- least_conn；：最少连接调度算法，当server拥有不同的权重时其为wlc 
- ip_hash：源地址hash调度方法 
- hash ：基于指定的key的hash表来实现对请求的调度，此处的key可以直接文本、变量或二者的组合 
- keepalive connections;：为每个worker进程保留的空闲的长连接数量； 
- weight=number # 权重，默认为1； 
- max_fails=number # 失败尝试最大次数；超出此处指定的次数时，server将被标记为不可用； 
- fail_timeout=time # 设置将服务器标记为不可用状态的超时时长； 
- max_conns # 当前的服务器的最大并发连接数； 
- backup # 将服务器标记为“备用”，即所有服务器均不可用时此服务器才启用； 
- down # 标记为“不可用”

# 七层负载均衡实验

```bash
node1作为反向代理服务器并配置七层代理
node2和node3作为web站点
[root@node1 ~]# cat /etc/nginx/conf.d/proxy.conf
upstream websers{
server 192.168.10.20 weight=2;
server 192.168.10.30 weight=1; #权重
}
server{
listen 8080;
server_name 172.16.0.10:8080;
location / {
proxy_pass http://websers;  #七层负载http即应用层
}
}
测试：
1.并发访问htt://172.16.0.10:8080
```

![image-20230205154452637](http://cdn.gtrinee.top/image-20230205154452637.png)

![image-20230205154508238](http://cdn.gtrinee.top/image-20230205154508238.png)

# 四层负载均衡

```bash
#默认没有安装四层负载均衡的模块，需要自己安装
yum install nginx-mod-stream -y

[root@node1 ~]# cat /etc/nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;
include /usr/share/nginx/modules/*.conf;
events {
worker_connections 1024;
}
stream {
	upstream sshsers{
		server 192.168.10.20:22;
        server 192.168.10.30:22;
        least_conn; #最少连接调度算法，当server拥有不同的权重时其为wlc 
        }
        server{ 
        listen 172.16.0.10:22222;
        proxy_pass sshsers;  
        }
}
测试：
1. 通过ssh root@172.16.0.10 22222测试
```

# 负载均衡算法

- 轮询(默认)： 每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器死机，自动剔 除故障系统，使 用户访问不受影响。 
- weight（轮询权值）：weight的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均 衡的情况下。或 者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源 
- ip_hash：每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器， 并且可以有效解决 动态网页存在的session共享问题。 
- fair：比 weight、ip_hash更加智能的负载均衡算法，fair算法可以根据页面大小和加载时间长短智 能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配。 Nginx本身不支持fair，如果需要这 种调度算法，则必须安装upstream_fair模块。 
- url_hash：按访问的URL的哈希结果来分配请求，使每个URL定向到一台后端服务器，可以进一步 提高后端缓存服 务器的效率。Nginx本身不支持url_hash，如果需要这种调度算法，则必须安装 Nginx的hash软件包。

# 四层、七层负载均衡的区别

[四层、七层负载均衡的区别 - 腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1082047)

# HAproxy负载均衡

## 介绍

HAProxy（高性能-代理）目前是一款使用广泛的软负载均衡器，**它可以实现TCP/HTTP的负载均衡。** 

特性： 免费开源 **最大并发量能达到5w 支持多种负载均衡算法**，同时支持session保持 支持虚拟主机 拥有服务监控页面，可以了解系统的实时运行状态 

**通常不做正向代理**，有更好的选择（Squid） 

通常不做缓存代理，有更好的选择（Varnish）

不会改变请求和响应报文 

不用于Web服务器 

不是基于数据包的负载均衡器，看不到ip数据包

## 安装部署

```bash
[root@node1 ~]# yum install haproxy -y
/etc/haproxy/haproxy.cfg # 主配置文件
/usr/lib/systemd/system/haproxy.service # 守护进程服务
默认配置：
[root@node1 ~]# cat /etc/haproxy/haproxy.cfg
```

- 全局配置

```bash
log 定义全局的syslog服务器
chroot 定义工作目录
pidfile 指定PID
maxconn 定义最大并发数
user 用户名
group 组名
daemon 以守护进程方式工作于后台
```

- 默认配置：用于所有其他配置段提供默认参数，这配置默认配置参数可由下一个defaults所重新设定

```bash
mode 运行模式或协议
log 定义每个实例启用事件和流量日志
option httplog
option dontlognull
option http-server-close
option forwardfor except 127.0.0.0/8
option redispatch
retries 3
timeout http-request 10s
timeout queue 1m
timeout connect 10s
timeout client 1m
timeout server 1m
timeout http-keep-alive 10s
timeout check 10s
maxconn 定义最大并发数
```

- 前端配置：定义一系列监听的套接字，这些套接字可接受客户端请求并与之连接

```bash
frontend main *:5000
	acl url_static path_beg -i /static /images /javascript /stylesheets # 测试请求的URL是否以指定的模式开头
	acl url_static path_end -i .jpg .gif .png .css .js # 测试请求的URL是否以<string>指定的模式结尾
	use_backend static if url_static
	default_backend app # 指定后端的名称（当use_backend 没能匹配）
```

- 后端配置：定义一系列“后端”服务器，代理将会将对应客户端的请求转发至这些服务器

```bash
backend static
    balance roundrobin
    server static 127.0.0.1:4331 check
backend app
    balance roundrobin
    server app1 127.0.0.1:5001 check
    server app2 127.0.0.1:5002 check
    server app3 127.0.0.1:5003 check
    server app4 127.0.0.1:5004 check

```

## 简单实验

```bash
[root@nginx1 ~]# cat /etc/haproxy/haproxy.cfg
global
    log 127.0.0.1 local2
    
    chroot /var/lib/haproxy
    pidfile /var/run/haproxy.pid
    maxconn 4000
    user haproxy
    group haproxy
    daemon
    stats socket /var/lib/haproxy/stats
defaults
    mode http
    log global
    option httplog
    option dontlognull
    option http-server-close
    option forwardfor except 127.0.0.0/8
    option redispatch
    retries 3
    timeout http-request 10s
    timeout queue 1m
    timeout connect 10s
    timeout client 1m
    timeout server 1m
    timeout http-keep-alive 10s
    timeout check 10s
    maxconn 3000
frontend main *:5000
	acl url_static path_beg -i /static /images /javascript
/stylesheets
    acl url_static path_end -i .jpg .gif .png .css .js .txt
    
    use_backend static if url_static
	default_backend app
    
backend static
	balance roundrobin
    server static 192.168.80.30:80 check
    
    
backend app
    balance roundrobin
    server app1 192.168.80.20:80 check

listen mysql_proxy
    bind 0.0.0.0:3306
    mode tcp
    balance source
    server mysqldb1 192.168.80.30:3306 check

listen ssh_proxy
    bind 0.0.0.0:22222 
    mode tcp  #tcp属于四层负载
    balance roundrobin  #负载算法
    server sshserver1 192.168.80.20:22 check
    server sshserver2 192.168.80.30:22 check
```

# LVS

## 介绍

- LB集群原理：当用户的请求过来时，会直接分发到Director Server上，然后它把用户的请求根据设 置好的调度算 法，智能均衡地分发到后端真正服务器(real server)上，为了保证用户请求数据一 致，需要共享存储。 
- LVS(Linux Virtual Server)Linux虚拟服务器。这是一个由章文嵩博士发起的一个开源项目，并且已 经是 Linux 内核标准的一部分。 
- LVS架构从逻辑上可分为调度层、Server集群层和共享存储。 
- 官方网是 http://www.linuxvirtualserver.org

## 组成

- ipvs(ip virtual server)：工作在内核空间，是真正生效实现调度的代码。 
- ipvsadm：工作在用户空间，负责为ipvs内核框架编写规则，定义谁是集群服务，而谁是后端真实 的服务器(Real Server) 
- 相关术语介绍 
  - DS：Directory Server，前端负载均衡节点 
  - RS：Real Server，后端真实工作服务器 
  - VIP：用户请求目标的IP地址 
  - DIP：内部主机通讯的IP地址 
  - RIP：后端服务器的IP地址 
  - CIP：客户端的IP地址

### 1.NAT模式

当用户访问服务器集群提供的服务时，发往虚拟IP地址的请求数据包（负载均衡器的外部IP地址）到达 负载均衡器。负载 均衡器检查数据包的目标地址和端口号。如果根据虚拟服务器规则表匹配虚拟服务器 服务，则通过调度算法从集群中选择 真实服务器，并将连接添加到记录已建立连接的哈希表中。然后， **将目标地址和数据包的端口重写为所选服务器的目标地 址和端口，并将数据包转发到服务器**。当传入数 据包属于此连接并且可以在哈希表中找到所选服务器时，将重写该数据包 并将其转发到所选服务器。当 回复数据包返回时，负载均衡器将数据包的源地址和端口重写为虚拟服务的源地址和端口。 连接终止或 超时后，连接记录将在哈希表中删除。



`NAT模式` 的运行方式如下图：

![NAT-ARCH](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d791ab228ee24802b061d4bb8462c248~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp)

**请求过程说明：**

- `client` 发送请求到 `LVS` 的 `VIP` 上，`Director` 服务器首先根据 `client` 的 IP 和端口从连接信息表中查询是否已经存在，如果存在就直接使用当前连接进行处理。否则根据负载算法选择一个 `Real-Server`（真正提供服务的服务器），并记录连接到连接信息表中，然后把 `client` 请求的目的 IP 地址修改为 `Real-Server` 的地址，将请求发给 `Real-Server`。
- `Real-Server` 服务器收到请求包后，发现目的 IP 是自己的 IP，于是处理请求，然后发送回复给 `Director` 服务器。 
- `Director` 服务器收到回复包后，修改回复包的的源地址为VIP，发送给 `client`。

> 上图中的蓝色连接线表示请求的数据流向，而红色连接线表示回复的数据流向。由于进出流量都需要经过 `Director` 服务器，所以 `Director` 服务器可能会成功瓶颈。

下面通过一幅图来说明一个请求数据包在 LVS 服务器中的地址变化情况：

![NAT-PACKAGE](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5db6c74328054194a87f89b18aade32e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp)

下面解释一下请求数据包的地址变化过程：

- client 向 LVS 集群发起请求，源IP地址和源端口为：`192.168.11.100:11021`，而目标IP地址和端口为：`192.168.10.10:80`。当 `Director` 服务器接收到 client 的请求后，会根据调度算法选择一台合适的 `Real-Server` 服务器，并且把请求数据包的目标IP地址和端口改为 `Real-Server` 服务器的IP地址和端口，并记录连接信息到连接信息表中，如上图选择的 `Real-Server` 服务器的IP地址和端口为：`192.168.1.2:80`。
- 当 `Real-Server` 服务器接收到请求后，对请求进行处理，处理完后会把数据包的源IP地址和端口跟目标IP地址和端口交互，然后发送给网关 `192.168.1.1`（也就是 `Director` 服务器）。
- `Director` 服务器接收到来自 `Real-Server` 服务器的回复数据，然后根据连接信息把源IP地址更改为虚拟IP地址。

> 由于 `Real-Server` 服务器需要把 `Director` 服务器设置为网关，所以 `Director` 服务器与 `Real-Server` 服务器需要部署在同一个网络下。



### 2. DR模式

此请求调度方法类似于IBM的NetDispatcher中实现的方法。虚拟IP地址由真实服务器和负载均衡器共享。负载均衡器的 接口也配置了虚拟IP地址，用于接受请求数据包，并直接将数据包路由到选定的服务器。所有真实服务器的非arp别名接口都配置了虚拟IP地址。或者将发往虚拟IP地址的数据包重定向到本 地套接字，以便真实服务器可以在本地处理数据包。 **负载均衡器和真实服务器必须通过HUB/Switch物 理连接其中一个接口**，负载均衡器只是将数据**帧的MAC地址更改为所选 服务器的MAC地址**，然后在LAN 上重新传输。这就是负载均衡器和每个服务器**必须通过同一个LAN**进行连接的原因。

`DR模式` 的运行方式如下图：

![DR-ARCH](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f4fb9ac5492d45bab81e3608bb62ade3~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp)

**请求过程说明：**

- `client` 发送请求到 `LVS` 的 `VIP` 上，`Director` 服务器首先根据 client 的 IP 和端口从连接信息表中查询是否已经存在，如果存在就直接使用当前连接进行处理。否则根据负载算法选择一个 `Real-Server`（真正提供服务的服务器），并记录连接到连接信息表中，然后通过修改请求数据包的目标 MAC 地址为 `Real-Server` 服务器的 MAC 地址（注意：**IP地址不修改**），并通过局域网把数据包发送出去。
- 由于 `Director` 服务器与 `Real-Server` 服务器在同一局域网中，所以通过数据包的目标 MAC 地址可以找到对应的 `Real-Server` 服务器（以太网协议），而 `Real-Server` 服务器接收到数据包后，会对数据包进行处理。
- `Real-Server` 服务器处理完请求后，把处理结果直接发送给 `client`，而不会通过 `Director` 服务器。

> 注意：`Real-Server` 服务器必须设置回环设备的 IP 地址为 VIP 地址，因为如果不设置 VIP，那么 `Real-Server` 服务器会认为这个数据包发送给本机的，从而丢弃这个数据包。

下面通过一幅图来说明一个请求数据包在 LVS 服务器中的地址变化情况：

![DR-PACKAGE](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/94adfc82325b4b8c9dd230b412a0b993~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp)

下面解释一下请求数据包的地址变化过程：

- client 向 LVS 集群发起请求，源IP地址和源端口为：`192.168.11.100:11021`，而目标IP地址和端口为：`192.168.10.10:80`。当 `Director` 服务器接收到 client 的请求后，会根据调度算法选择一台合适的 `Real-Server` 服务器，并且把请求数据包的目标 MAC 地址改为 `Real-Server` 服务器的 MAC 地址，并记录连接信息到连接信息表中，然后通过局域网把数据包发送出去。
- 当 `Real-Server` 服务器处理完请求后，会把处理结果直接发送给 `client`。如果 `Real-Server` 服务器与 `client` 在同一个局域网，那么直接通过把目标 MAC 地址修改为 `client` 的 MAC 地址。否则，通过把目标 MAC 地址修改为路由器的 MAC 地址，然后通过路由器发送出去。

> 从上图可以看出，整个请求过程中，数据包只有 MAC 地址发生变化。 另外，由于 `DR模式` 只有入口需要经过 `Director` 服务器，而出口不需要经过 `Director` 服务器，所以性能比 `NAT模式` 要高。



### 3. TUN模式

![这里写图片描述](https://img-blog.csdn.net/20170914232857937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvUnVubmluZ19mcmVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在LVS（NAT）模式的集群环境中，由于所有的数据请求及响应的数据包都需要经过LVS调度器转发，**如 果后端服务器的 数量大于10台**，则**调度器就会成为整个集群环境的瓶颈**。我们知道，**数据请求包往往远 小于响应数据包的大小**。因为响 应数据包中包含有客户需要的具体数据，所以LVS（TUN）的思路就是 **将请求与响应数据分离，让调度器仅处理数据请 求，而让真实服务器响应数据包直接返回给客户端。**

LVS/TUN工作模式拓扑结构如图3所示。其中，IP隧道（IP tunning）是一种数据**包封装技术，**它可以将 原始数据包封装并添加新的包头（内容包括新的源地址及端口、目标地址及 端口），从而实现将一个**目 标为调度器的VIP地址的数据包封装**，通过隧道转发给后端的真实服务器（Real Server），通 过将客户 端发往调度器的原始数据包封装，并在其基础上添加新的数据包头（修改目标地址为调度器选择出来的 真实服务 器的IP地址及对应端口），LVS（TUN）模式要求真实服务器可以直接与外部网络连接，真实 服务器在收到请求数据包后 直接给客户端主机响应数据。

与dr区别就是增加一块报头

- 特点
  - RIP、VIP、DIP全是公网地址 
  - RS的网关不会也不可能指向DIP 
  - 所有的请求报文经由Director Server，但响应报文必须不能进过Director Server 
  - 不支持端口映射 RS的系统必须支持隧道

## LVS实验部署

### NAT实践

- 环境准备

```bash
DIP:192.168.10.10/24 #内部主机通讯的IP地址
VIP:172.16.0.10/16   #后端服务器的IP地址 
RIP1:192.168.10.20 GATEWAY:192.168.10.10
RIP2:192.168.10.30 GATEWAY:192.168.10.10
```

- DS部署

```bash
1.安装ipvsadm工具
[root@node1 ~]# yum install ipvsadm -y
2.打开转发，配置nat
[root@node1 ~]# echo 1 >> /proc/sys/net/ipv4/ip_forward
[root@node1 ~]# iptables -t nat -F
[root@node1 ~]# iptables -t nat -A POSTROUTING -s 192.168.10.0/24 -o ens37 -j MASQUERADE
3.设置wrr策略和添加RS主机
[root@node1 ~]# ipvsadm -A -t 172.16.0.10:80 -s wrr
[root@node1 ~]# ipvsadm -a -t 172.16.0.10:80 -r 192.168.10.20:80 -m -w 1
[root@node1 ~]# ipvsadm -a -t 172.16.0.10:80 -r 192.168.10.30:80 -m -w 1
4.查看策略信息
[root@node1 ~]# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 172.16.0.10:80 wrr
-> 192.168.10.20:80 Masq 1 0 0
-> 192.168.10.30:80 Masq 1 0 0
```

- RS部署

```bash
# RS的网关必须指向DS
[root@node2 ~]# echo "RS2" > /usr/share/nginx/html/index.html
[root@node3 ~]# echo "RS3" > /usr/share/nginx/html/index.html
[root@node1 ~]# curl 172.16.0.10
RS1
[root@node1 ~]# curl 172.16.0.10
RS2
注意：selinux策略和防火墙！
```

### ipvsadm命令

```bash
ipvsadm：
管理集群服务
    添加：-A -t|u|f service-address [-s scheduler]
    -t: TCP协议的集群
    -u: UDP协议的集群
    service-address: IP:PORT
    -f: FWM: 防火墙标记
    service-address: Mark Number
修改：-E
删除：-D -t|u|f service-address
管理集群服务中的RS

    添加：-a -t|u|f service-address -r server-address [-g|i|m] [-w weight]
    -t|u|f service-address：事先定义好的某集群服务
    -r server-address: 某RS的地址，在NAT模型中，可使用IP：PORT实现端口映射；
    [-g|i|m]: LVS类型
    -g: DR模型
    -i: TUN模型
    -m: NAT模型
    [-w weight]: 定义服务器权重
修改：-e
删除：-d -t|u|f service-address -r server-address
查看
    -L|l
    -n: 数字格式显示主机地址和端口
    --stats：统计数据
    --rate: 速率
    --timeout: 显示tcp、tcpfin和udp的会话超时时长
    -c: 显示当前的ipvs连接状况
    删除所有集群服务
    -C：清空ipvs规则
保存规则
    -S
    # ipvsadm -S > /path/to/somefile
    载入此前的规则：
    -R
    # ipvsadm -R < /path/form/somefile
```

## DR实践

- 环境准备

```bash
DIP:192.168.80.10/24
RIP1:192.168.80.20
RIP2:192.168.80.30
VIP:192.168.80.100
```

- DS部署

```bash
[root@node1 ~]# ifconfig ens33:0 192.168.80.100 broadcast 192.168.80.255
netmask 255.255.255.0 up
[root@node1 ~]# route add -host 192.168.80.100 dev ens33:0
[root@node1 ~]# ipvsadm -A -t 192.168.80.100:80 -s wrr
[root@node1 ~]# ipvsadm -a -t 192.168.80.100:80 -r 192.168.80.20:80 -g -w 2
[root@node1 ~]# ipvsadm -a -t 192.168.80.100:80 -r 192.168.10.30:80 -g -w 1
```

- RS部署

```bash
[root@node2 ~]# cat rs.sh
#!/bin/bash
vip=192.168.80.100
ifconfig lo:0 $vip broadcast 192.168.80.255 netmask 255.255.255.255 up
route add -host $vip lo:0
echo "1" >/proc/sys/net/ipv4/conf/lo/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/lo/arp_announce
echo "1" >/proc/sys/net/ipv4/conf/all/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/all/arp_announce
在所有RS上部署！
```

测试 

不要在DS上直接进行测试，可能会因为网络的问题导致丢包



## 调度算法 

### 轮循(Round Robin)

这种方法会将收到的请求循环分配到服务器集群中的每台机器，即有效服务器。如果使用这种方式，所有的标记进入虚拟服务的服务器应该有相近的资源容量以及负载形同的应用程序。如果所有的服务器有相同或者相近的性能那么选择这种方式会使服务器负载形同。基于这个前提，轮循调度是一个简单而有效的分配请求的方式。然而对于服务器不同的情况，选择这种方式就意味着能力比较弱的服务器也会在下一轮循环中接受轮循，即使这个服务器已经不能再处理当前这个请求了。这可能导致能力较弱的服务器超载。

### 加权轮循(Weighted Round Robin)

这种算法解决了简单轮循调度算法的缺点：传入的请求按顺序被分配到集群中服务器，但是会考虑提前为每台服务器分配的权重。管理员只是简单的通过服务器的处理能力来定义各台服务器的权重。例如，能力最强的服务器A给的权重是100，同时能力最低的服务器给的权重是50。这意味着在服务器B接收到第一个请求之前前，服务器A会连续的接受到2个请求，以此类推。

### 最少连接数(Least Connection)

以上两种方法都没有考虑的是系统不能识别在给定的时间里保持了多少连接。因此可能发生，服务器B服务器收到的连接比服务器A少但是它已经超载，因为服务器B上的用户打开连接持续的时间更长。这就是说连接数即服务器的负载是累加的。这种潜在的问题可以通过"最少连接数"算法来避免：传入的请求是根据每台服务器当前所打开的连接数来分配的。即活跃连接数最少的服务器会自动接收下一个传入的请求。接本上和简单轮询的原则相同：所有拥有虚拟服务的服务器资源容量应该相近。值得注意的是，在流量率低的配置环境中，各服务器的流量并不是相同的，会优先考虑第一台服务器。这是因为，如果所有的服务器是相同的，那么第一个服务器优先，直到第一台服务器有连续的活跃流量，否则总是会优先选择第一台服务器。

### 最少连接数慢启动时间(Least Connection Slow Start Time)

对最少连接数和带权重的最小连接数调度方法来说，当一个服务器刚加入线上环境是，可以为其配置一个时间段，在这段时间内连接数是有限制的而且是缓慢增加的。这为服务器提供了一个'过渡时间'以保证这个服务器不会因为刚启动后因为分配的连接数过多而超载。这个值在L7配置界面设置。

### 加权最少连接(Weighted Least Connection)

如果服务器的资源容量各不相同，那么"加权最少连接"方法更合适：由管理员根据服务器情况定制的权重所决定的活跃连接数一般提供了一种对服务器非常平衡的利用，因为他它借鉴了最少连接和权重两者的优势。通常，这是一个非常公平的分配方式，因为它使用了连接数和服务器权重比例;集群中比例最低的服务器自动接收下一个请求。但是请注意，在低流量情况中使用这种方法时，请参考"最小连接数"方法中的注意事项。

### **基于代理的自适应负载均衡(Agent Based Adaptive Balancing)**

除了上述方法之外，负载主机包含一个自适用逻辑用来定时监测服务器状态和该服务器的权重。对于非常强大的"基于代理的自适应负载均衡"方法来说，负载主机以这种方式来定时检测所有服务器负载情况：每台服务器都必须提供一个包含文件，这个文件包含一个0~99的数字用来标明改服务器的实际负载情况(0=空前，99=超载，101=失败，102=管理员禁用)，而服务器同构http get方法来获取这个文件;同时对集群中服务器来说，以二进制文件形式提供自身负载情况也是该服务器工作之一，然而，并没有限制服务器如何计算自身的负载情况。根据服务器整体负载情况，有两种策略可以选择：在常规的操作中，调度算法通过收集的服务器负载值和分配给该服务器的连接数的比例计算出一个权重比例。因此，如果一个服务器负载过大，权重会通过系统透明的作重新调整。和加权轮循调度方法一样，不正确的分配可以被记录下来使得可以有效的为不同服务器分配不同的权重。然而，在流量非常低的环境下，服务器报上来的负载值将不能建立一个有代表性的样本;那么基于这些值来分配负载的话将导致失控以及指令震荡。因此，在这种情况下更合理的做法是基于静态的权重比来计算负载分配。当所有服务器的负载低于管理员定义的下限时，负载主机就会自动切换为加权轮循方式来分配请求;如果负载大于管理员定义的下限，那么负载主机又会切换回自适应方式。

### 固定权重(Fixed Weighted)

最高权重只有在其他服务器的权重值都很低时才使用。然而，如果最高权重的服务器下降，则下一个最高优先级的服务器将为客户端服务。这种方式中每个真实服务器的权重需要基于服务器优先级来配置。

### 加权响应(Weighted Response)

流量的调度是通过加权轮循方式。加权轮循中所使用的权重是根据服务器有效性检测的响应时间来计算。每个有效性检测都会被计时，用来标记它响应成功花了多长时间。但是需要注意的是，这种方式假定服务器心跳检测是基于机器的快慢，但是这种假设也许不总是能够成立。所有服务器在虚拟服务上的响应时间的总和加在一起，通过这个值来计算单个服务物理服务器的权重;这个权重值大约每15秒计算一次。

### 源IP哈希(Source IP Hash)

这种方式通过生成请求源IP的哈希值，并通过这个哈希值来找到正确的真实服务器。这意味着对于同一主机来说他对应的服务器总是相同。使用这种方式，你不需要保存任何源IP。但是需要注意，这种方式可能导致服务器负载不平衡。



# Keepalived

Keepalived一个基于[VRRP](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/VRRP) 协议来实现的 LVS 服务高可用方案，可以利用其来解决单点故障。一个LVS服务会有2台服务器运行Keepalived，一台为主服务器（MASTER），一台为备份服务器（BACKUP），但是对外表现为一个`虚拟IP`，主服务器会发送特定的消息给备份服务器，当备份服务器收不到这个消息的时候，即主服务器宕机的时候， 备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性。

## 简介

- Keepalived是Linux下一个轻量级别的高可用解决方案，Keepalived起初是为LVS设计的，专门用 **来监控集群系统 中各个服务节点的状态**，如果某个服务器节点出现故障，**Keepalived将检测到后 自动将节点从集群系统中剔除**。 
- 后来Keepalived又加入了VRRP的功能，VRRP（VritrualRouterRedundancyProtocol,虚拟路由冗 余协议)出现的目 的是**解决静态路由出现的单点故障问题**，通过VRRP可以实现网络不间断稳定运 行，因此Keepalvied一方面具有服 务器状态检测和故障隔离功能，另外一方面也有HAcluster功 能。 
- **健康检查和失败切换**是keepalived的两大核心功能。所谓的**健康检查**，就是采用**tcp三次握手， icmp请求，http请 求**，udp echo请求等方式对负载均衡器后面的实际的服务器(通常是承载真实业 务的服务器)进行保活；而失败切换 主要是应用于配置了主备模式的负载均衡器，利用VRRP维持主 备负载均衡器的**心跳**，当主负载均0衡器出现问题时， 由备负载均衡器承载对应的业务，从而在最大 限度上减少流量损失，并提供服务的稳定性。

- 工作原理 
  - 网络层：通过ICMP协议向后端服务器集群中发送数据报文 
  - 传输层：利用TCP协议的端口连接和扫描技术检测后端服务器集群是否正常 
  - 应用层：自定义keepalived工作方式（脚本）

## VRRP协议

![2387773-20210527185130542-483841541.png](https://ucc.alicdn.com/pic/developer-ecology/e5dd2d82443c4dd2a47a892c403d4f84.png)

- VRRP协议是一种容错的主备模式的协议，保证当主机的下一跳路由出现故障时，由另一台路由器 来代替出现故障的 路由器进行工作，通过VRRP可以在网络发生故障时透明的进行设备切换而不影 响主机之间的数据通信。 
- 虚拟路由器：VRRP组中所有的路由器，拥有虚拟的IP+MAC(00-00-5e-00-01-VRID)地址

- 主路由器：虚拟路由器内部通常只有一台物理路由器对外提供服务，主路由器是由选举算法产生， 对外提供各种网络功能。 
- 备份路由器：VRRP组中除主路由器之外的所有路由器，不对外提供任何服务，只接受主路由的通 告，当主路由器挂掉之后，重新进行选举算法接替master路由器。 
- 选举机制 
  - 优先级 
  - 抢占模式下，一旦有优先级高的路由器加入，即成为Master 
  - 非抢占模式下，只要Master不挂掉，优先级高的路由器只能等待 
- 三种状态 
  - Initialize状态：系统启动后进入initialize状态 
  - Master状态 
  - Backup状态





![165701_aAp2_2540936.png](http://static.oschina.net/uploads/space/2016/0418/165701_aAp2_2540936.png)



大致分两层结构：用户空间user space和内核空间kernel space
1：IPVS：IP虚拟服务器（IP Virtual Server），是一种提供负载平衡功能的技术
2：NetLink：提供高级路由及其他相关的网络功能
3：WatchDog：负责监控checkers和VRRP进程的状况
4：Checkers：负责真实服务器的健康检查，是[keepalived](https://so.csdn.net/so/search?q=keepalived&spm=1001.2101.3001.7020)最主要的功能。可以没有
VRRP Stack,但健康检查healthchecking是一定要有的。
5：VRRP Stack：负责负载均衡器之间的失败切换FailOver。如果只用一个负载均衡
器，则VRRP不是必须的。
6：IPVS wrapper：用来发送设定的规则到内核的IPVS部分
7：Netlink Reflector：用来设定VRRP的vip地址等
8：控制面板：对配置文件的编译和解析。Keepalived不是一次性解析所有的配置文
件，而是用到一个模块解析一个，因此可以在每个模块看到XXX_parser.c这样的
文件





# keepalived+nginx实验

- 环境准备

```bash
node1（Nginx1）：192.168.10.10
node2（Nginx2）：192.168.10.20
node3（WEB1）：192.168.10.30
node4（WEB2）：192.168.10.40
VIP：192.168.10.100
```

- web部署

```bash
在node3和node4执行下面的脚本：
#!/bin/bash
yum install net-tools httpd -y
systemctl stop firewalld
setenforce 0
echo "<h1>This is RS1</h1>" > /var/www/html/index.html # 修改不同的主页以便测试！
systemctl start httpd
```

- nginx部署

```bash
node1和node2节点执行以下脚本：
#!/bin/bash
systemctl stop firewalld
setenforce 0
yum install nginx -y
cat > /etc/nginx/conf.d/proxy.conf << EOF
upstream websers{
    server 192.168.10.30;
    server 192.168.10.40;
}
server{
    listen 8080;
    server_name 192.168.10.10;
    location / {
	    proxy_pass http://websers;
}
}
EOF
nginx -s reload
```

- keepalived部署

```bash
在node1和node2节点执行以下脚本：
#!/bin/bash
yum install keepalived -y
mv /etc/keepalived/keepalived.conf{,.bak}
cat > /etc/keepalived/keepalived.conf << EOF
! Configuration File for keepalived
global_defs {
	router_id node1 # node2修改
}
vrrp_instance VI_1 {  #定义vrrp实例
	state MASTER # node2节点BACKUP
	interface ens33  #vrrp配置在ens33
    virtual_router_id 10  
    priority 100 # node2节点优先级小于100 
    advert_int 1  #检查时间间隔
    authentication {   #认证类型
    auth_type PASS  
    auth_pass 1111   #密码
    }
    virtual_ipaddress {   
    192.168.10.100   #虚拟ip 如果node1 keepalived出问题自动IP漂移到node2 进行负载均衡
    }
}
```

- 自定义脚本检测nginx服务是否正常

```bash
[root@node1 ~]# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
	router_id node1  #路由id 
}
# 定义script 脚本如果nginx报错自动重启 脚本在底下
vrrp_script chk_http_port {
    script "/usr/local/src/check_nginx_pid.sh"
    interval 1 #时间间隔一秒检查一次
    weight -2 # 优先级-2  使得由master变成backup节点 vip漂移到node2
  
}
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 10
    priority 100
    advert_int 1
    authentication {
    auth_type PASS
    auth_pass 1111
}
	#调用script脚本
    track_script {
    chk_http_port
    }
    virtual_ipaddress {
    192.168.10.100
    }
}


[root@node1 ~]# cat /usr/local/src/check_nginx_pid.sh
#!/bin/bash
# 当nginx正常的时候状态码为0
# 当nginx不正常时候状态码为1
# 因为当退出状态码为非0的时候会执行切换
nginx_process_number=`ps -C nginx --no-header | wc -l`
if [ $nginx_process_number -eq 0 ];then
    # systemctl restart nginx
    nginx_process_number=`ps -C nginx --no-header | wc -l`
    if [ $nginx_process_number -eq 0 ];then
    	exit 1
    else
 	   exit 0
    fi
else
    exit 0
fi

```



# Keepalived+HAProxy+mysql双主实验

- 环境准备

```bash
node1（HAProxy1）：192.168.10.10
node2（HAProxy2）：192.168.10.20
node3（Mysql1）：192.168.10.30
node4（Mysql2）：192.168.10.40
VIP：192.168.10.99
```

- mysql部署

```bash
在node3和node4执行以下脚本：
#!/bin/bash
Ip_addr="192.168.10.40" # 修改为对端的node地址
User_pwd="000000"
systemctl stop firewalld
setenforce 0
yum install mariadb-server -y
sed -i '/^\[mysqld\]$/a\binlog-ignore = information_schema'
/etc/my.cnf.d/server.cnf
sed -i '/^\[mysqld\]$/a\binlog-ignore = mysql' /etc/my.cnf.d/server.cnf
sed -i '/^\[mysqld\]$/a\skip-name-resolve' /etc/my.cnf.d/server.cnf
sed -i '/^\[mysqld\]$/a\auto-increment-increment = 1'
/etc/my.cnf.d/server.cnf # 注意node4节点上必须不同
sed -i '/^\[mysqld\]$/a\log-bin = mysql-bin' /etc/my.cnf.d/server.cnf
sed -i '/^\[mysqld\]$/a\auto_increment_offset = 1' /etc/my.cnf.d/server.cnf
# 注意node4节点上必须不同
sed -i '/^\[mysqld\]$/a\server-id = 1' /etc/my.cnf.d/server.cnf # 注意node4节点上必须不同

systemctl restart mariadb
mysql -uroot -e "grant replication slave on *.* to 'repuser'@'$Ip_addr'
identified by '$User_pwd';"

查询node3节点master状态：
MariaDB [(none)]> show master status;
+------------------+----------+--------------+--------------------------+
| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+--------------------------+
| mysql-bin.000003 | 402 | | mysql,information_schema |
+------------------+----------+--------------+--------------------------+
查询node4节点master状态
MariaDB [(none)]> show master status;
+------------------+----------+--------------+--------------------------+
| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+--------------------------+
| mysql-bin.000003 | 407 | | mysql,information_schema |
+------------------+----------+--------------+--------------------------+


#在node3节点执行连接命令：

MariaDB [(none)]> change master to
master_host='192.168.10.40',master_port=3306,master_user='repuser',master_password='000000',master_log_file='mysqlbin.000003',master_log_pos=407;
MariaDB [mysql]> start slave;
在node4节点执行连接命令：
MariaDB [(none)]> change master to
master_host='192.168.10.30',master_port=3306,master_user='repuser',master_pa
ssword='000000',master_log_file='mysql-bin.000003',master_log_pos=402;
MariaDB [mysql]> start slave;
查看从节点状态： show slave status \G; 观察IO和SQL线程是否为YES

MariaDB [(none)]> show slave status \G;
Slave_IO_Running: Yes
Slave_SQL_Running: Yes
测试：
1. 在node3上创建mydb数据库
2. 在node4上在test数据库中创建test表
3. 最终要实现node3和node4上保持数据同步
```

- HAProxy部署

```bash
在node1和node2上执行以下脚本：
#!/bin/bash
yum install haproxy -y
mv /etc/haproxy/haproxy.cfg{,.bak}
cat > /etc/haproxy/haproxy.cfg << EOF
global
    log 127.0.0.1 local2
    chroot /var/lib/haproxy
    pidfile /var/run/haproxy.pid
    maxconn 4000
    user haproxy
    group haproxy
    daemon
    stats socket /var/lib/haproxy/stats

listen mysql_proxy
    bind 0.0.0.0:3306
    mode tcp
    balance source
    server mysqldb1 192.168.10.30:3306 weight 1 check
    server mysqldb2 192.168.10.40:3306 weight 2 check

listen stats
    mode http
    bind 0.0.0.0:8080
    stats enable
    stats uri /dbs
    stats realm haproxy\ statistics
    stats auth admin:admin
EOF
systemctl start haproxy
```

- keepalived部署

```bash
node1上执行以下脚本：
#!/bin/bash
yum install keepalived -y
mv /etc/keepalived/keepalived.conf{,.bak}
cat > etc/keepalived/keepalived.conf << EOF
! Configuration File for keepalived
global_defs {
	router_id node1
}
vrrp_script chk_http_port {
    script "/usr/local/src/check_proxy_pid.sh"
    interval 1
    weight -2
}
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 10
    priority 100
    advert_int 1
    authentication {
    auth_type PASS
    auth_pass 1111
    }
    track_script {
    chk_http_port
    }
    virtual_ipaddress {
    192.168.10.100
    }
}
EOF

systemctl start keepalived
node2上执行以下脚本：
#!/bin/bash
yum install keepalived -y
mv /etc/keepalived/keepalived.conf{,.bak}
cat > etc/keepalived/keepalived.conf << EOF
! Configuration File for keepalived
global_defs {
	router_id node2
}
vrrp_instance VI_1 {
	state MASTER
    interface ens33
    virtual_router_id 10
    priority 99
    advert_int 1
    authentication {
    auth_type PASS
    auth_pass 1111
    }
    virtual_ipaddress {
    192.168.10.100
    }
    }
EOF
systemctl start keepalived

[root@node1 src]# cat check_proxy_pid.sh
#!/bin/bash
A=`ps -C haproxy --no-header | wc -l`
if [ $A -eq 0 ];then
	exit 1
else
	exit 0
fi

```



## Keepalived+LVS实验

- 环境准备

```bash
node1（DS1）：192.168.10.10
node2（DS2）：192.168.10.20
node3（RS1）：192.168.10.30
node4（RS2）：192.168.10.40
VIP：192.168.10.99（单主模型）
```

- RS部署

```bash
在node3和node4执行下面的脚本：
#!/bin/bash
yum install net-tools httpd -y
systemctl stop firewalld
setenforce 0
vip="192.168.10.99"
mask="255.255.255.255"
ifconfig lo:0 $vip broadcast $vip netmask $mask up
route add -host $vip lo:0
echo "1" >/proc/sys/net/ipv4/conf/lo/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/lo/arp_announce
echo "1" >/proc/sys/net/ipv4/conf/all/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/all/arp_announce
echo "<h1>This is RS1</h1>" > /var/www/html/index.html # 修改不同的主页以便测试！
systemctl start httpd
```

- DR部署

```bash
[root@node1 ~]# yum install keepalived ipvsadm -y
[root@node1 ~]# systemctl stop firewalld
[root@node1 ~]# setenforce 0
[root@node1 keepalived]# mv keepalived.conf{,.bak}
[root@node1 keepalived]# cat keepalived.conf
! Configuration File for keepalived
global_defs {
	router_id node1 # 设置lvs的id，一个网络中应该唯一
}
vrrp_instance VI_1 {
    state MASTER # 指定Keepalived的角色
    interface ens33 # 网卡
    virtual_router_id 10 # 虚拟路由器ID，主备需要一样
    priority 100 # 优先级越大越优，backup路由器需要设置比这小！
    advert_int 1 # 检查间隔1s
    authentication {
    auth_type PASS
auth_pass 1111
}
virtual_ipaddress {
192.168.10.99 # 定义虚拟IP地址，可以定义多个
}
}
# 定义虚拟主机，对外服务的IP和port
virtual_server 192.168.10.99 80 {
    delay_loop 6 # 设置健康检查时间，单位是秒
    lb_algo wrr # 负责调度算法
    lb_kind DR # LVS负载均衡机制
    persistence_timeout 0
    protocol TCP
    # 指定RS主机IP和port
    real_server 192.168.10.30 80 {
    weight 2
# 定义TCP健康检查
    TCP_CHECK {
    connect_timeout 10
    nb_get_retry 3
    delay_before_retry 3
    connect_port 80
    }
    }
real_server 192.168.10.40 80 {
weight 1
TCP_CHECK {
connect_timeout 10
nb_get_retry 3
delay_before_retry 3
connect_port 80
}
}
}

```







