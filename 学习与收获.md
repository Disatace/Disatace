飞书文档指南

https://www.feishu.cn/hc/zh-CN/categories-detail?category-id=6998429212675686401

# **项目中学习**

https://github.com/opencv/cvat/blob/develop/docker-compose.yml

https://github.com/Disatace/Disatace/blob/main/30.Kubernetes.md

https://github.com/kubernetes/kompose

软件应用开发的经典模型有这样几个环境：开发环境(development)、集成环境(integration)、测试环境(testing)、QA验证，模拟环境(staging)、生产环境(production)。

开发环境（dev）：开发环境是程序猿们专门用于开发的服务器，配置可以比较随意，为了开发调试方便，一般打开全部错误报告。

测试环境 （test）：一般是克隆一份生产环境的配置，一个程序在测试环境工作不正常，那么肯定不能把它发布到生产机上。

生产环境（prod）：是值正式提供对外服务的，一般会关掉错误报告，打开错误日志。通常说的真实环境。

通常一个web项目都需要一个staging环境，一来给客户做演示，二来可以作为production server的一个“预演”，正式发布新功能前能及早发现问题（特别是gem的依赖问题，环境问题等）。

staging server  ： 可以理解为production环境的镜像， QA在staging server上对新版本做最后一轮verification, 通过后才能deploy到产品线上. 有点网讯SERCM流程里面的SDA验证用的环境， 尽最大可能来模拟产品线上的环境(硬件，网络拓扑结构，数据库数据)

## CI/CD是什么？

CI(Continuous integration，中文意思是持续集成)是一种软件开发时间。持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。借用网络图片对CI加以理解。

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=NzVmYjViOTU3ZGYwOTBjZTg3NWZhZTI5N2FkZmFiMzNfQ1VuTkRZOHF1RHFZV3BROWcxWkVRZ1hURG5MS0wzdnZfVG9rZW46Ym94Y25VSWVIWDV0RnprTXR1T0dwb2V0cE5kXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

CD(Continuous Delivery， 中文意思持续交付)是在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境(类生产环境)中。比如，我们完成单元测试后，可以把代码部署到连接数据库的Staging环境中更多的测试。如果代码没有问题，可以继续手动部署到生产环境。下图反应的是CI/CD 的大概工作模式。

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=M2MyZGVmZTQxMjZkYmEyZWQyMWRlMWFkYTk2ZWUzNTBfYzBPQXUwbGQ5bXRoVFo5aFdVNHpCckhZbmpOd1laY25fVG9rZW46Ym94Y254MFZsOUtWcEtscFl1TldBdEZtRkNoXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# pod内docker all in one 与 one by one 的优缺点

- 单容器Pod，最常见的应用方式。
- 多容器Pod，对于多容器Pod，Kubernetes会保证所有的容器都在同一台物理主机或虚拟主机中运行。多容器Pod是相对高阶的使用方式，**除非应用****耦合****特别严重**，一般不推荐使用这种方式。一个Pod内的容器共享IP地址和端口范围，容器之间可以通过 localhost 互相访问。

## **为什么使用多容器****pod****?**

比如有一个application需要多个容器运行在相同Host上时，最好的选择就是多容器pod，尽管这会违反“One process per container” 规则，也不利于排查问题，但是利大于弊，例如团队内部可以重用更精细的容器。

## **使用多容器****pod****的好处？**

（1）同一个pod内的container可使用相同的网络命名空间、共享存储卷、使用相同的IPC命名空间。彼此间可有效进行通信，从而确保数据局部性。

（2）可将多个紧密耦合的application容器作为一个unit来管理

（3）所有容器有相同的生命周期，应在同一个节点上运行。

**多容器****Pod****的特征**：

运行在一个`pod`节点上的多个容器，它们共享的资源有：

- 相同的`Linux`命名空间（相同`IP`地址和端口空间，容器间互相访问）
- 相同 `IPC`名称空间
- 可以使用相同的共享卷

**使用场景**：应用由一个主进程和一个/多个辅助进程组成。

# **K8S: 有状态** **vs** **无状态****服务**

1. **1. 基本概念**

- 无状态服务

无状态服务**不会在本地存储持久化****数据**.多个服务实例对于同一个用户请求的响应结果是完全一致的.这种多服务实例之间是**没有依赖关系**,比如web应用,在k8s控制器 中动态启停无状态服务的pod并不会对其它的pod产生影响.

- 有状态服务

有状态服务**需要在本地存储持久化****数据**,典型的是**分布式****数据库****的应用,**分布式节点实例之间有依赖的拓扑关系.比如,主从关系. 如果K8S**停止**分布式集群中任 一实例pod,就可能会导致数据**丢失**或者集群的crash.

## **Deployment部署的问题?**

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=MjQ0MjI3M2QwMTE2OWEwOTQxNTA4ZWY5Y2JjOThhMDFfY0xzVUpJWDRWb0t2UzZqd015dmdKanhqYlVDQmhndktfVG9rZW46Ym94Y250SGRZSjNzM2MxaUJBeWN1WnZIQkdlXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

Deployment被设计用来**管理无状态****服务****的****pod**,每个pod完全一致.什么意思呢?

- 无状态服务内的多个Pod创建的顺序是没有顺序的.
- 无状态服务内的多个Pod的名称是随机的.pod被重新启动调度后,它的名称与IP都会发生变化.
- 无状态服务内的多个Pod背后是**共享存储**的.

### **2.1.新的问题**

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=MTBjYjYxZDQ0MWQ4Y2E4ZTkyYTQxNmNlOGFiY2IzM2ZfUDdveVpCdmlGREhqbmtyczE2bnl0Rmg3N1pJcmFON1pfVG9rZW46Ym94Y243b2JaQ09LU21rZFI3amhuSnYxV0ZlXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

- 对于数据库有状态的服务容器编排,Deployment解决方案就变得无能为力了.
- 比如,Redis是主从的架构,只能允许集群中出现一个主节点提供写,其它节点提供读能力.如果同时出现二个主节点后,必须会出现并发写的 操作,进一步导致集群写数据的不一致.
- 所以问题来了,针对Redis这种有状态的服务,它管理的多个Pod(代表master/slave角色)必须有自己**独立的持久化存储组件.** 
  - 有状态的服务Pod是用来运行有状态应用的,其在数据卷上存储的数据非常重要,因为**Stateful就是要依赖存储数据卷上对每个Pod的状态进行建模与存储.**

所以K8S提供了一个新的工具——**StatefulSet**来统一解决问题.

1. **3. Stateful如何解决问题?**

Deployment组件是为无状态服务而设计的,其中的**Pod名称,主机名,存储都是随机,不稳定的**,并且Pod的创建与销毁也是无序的.这个设计决定了无状态服务并 **不适合****数据库****领域的应用.**

而Stateful管理有状态的应用,它的Pod有如下特征:

- 唯一性: 每个Pod会被分配一个唯一序号.
- 顺序性: Pod启动,更新,销毁是按顺序进行.
- 稳定的网络标识: Pod主机名,DNS地址不会随着Pod被重新调度而发生变化.
- 稳定的持久化存储: Pod被重新调度后,仍然能挂载原有的PV,从而保证了数据的完整性和一致性.

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=NDE2YWE5YjYwZTI2ZTg3NDQ3MTllNjk5NWVmY2JiYjhfNm8xR2J2d1dzNjFNdWl3eVo0SVJGQkZqQ2ZiRkFrMUtfVG9rZW46Ym94Y25FYmVXYTc3aGlVR1N0QlZ1aTNKNlFlXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

### **3.1. 如何理解稳定网络标识?**

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=ODVhZWQwZDZhNzVjODRmM2MxOGFmNTIyMDFmOTUzMDFfZVloczdhZjd0UE14Y1JtWEJ3UXVOWnpDYnRhSzdjdklfVG9rZW46Ym94Y25LcWVla2REMEJnZEdjSWE0d3NwbnJmXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

创建名为test-redis-pod的Stateful模型,根据你配置的Replica=3的设置,K8S会创建三个Pod,依次命名为: test-redis-pod-0; test-redis-pod-1; test-redis-pod-2

K8S为有状态的服务Pod分配稳定的网络标识,具体实现基于test-redis-pod-0名称,借助Headless DNS进行如下解析,获取后端其中一个Pod的地址. 

$(pod name).$(service name).$(namespace).svc.cluster.local

下面是一个通过Pod名称访问Redis集群的Master节点地址的方法.

session.save_path = "tcp://test-redis-pod-0.test-redis-service.default.svc.cluster.local:6379"

**现在回答如下二个问题:**

- 在Redis Pod内部,主从节点之间数据同步的需求,Slave节点对应的配置文件中需要一个稳定的Master地址.下边脚本就是稳定访问test-redis-pod-0 名称来间接获得Redis Master节点IP地址,然后写入到Redis Slave的配置文件中,这样后续Slave节点与Master节点才能完成增量数据的同步. 

```undefined
if [ "${server_host}" != "test-redis-pod-0" ];then
      #echo "server-count: ${server_counts}" >> /data/test.log

      while [ -z "${master_address}" ];do
          echo "master_address is not available, ${master_address} waiting for redis master..." >> /data/test.log
          master_address=$(replication_master_address)
          sleep 1s
      done
  fi

  echo "master_address: $(master_address)" >> /data/test.log
  if [ ! -z "$master_address" ]; then
      printf "\nslaveof %s 6379\n" "$master_address" >> $conf
  fi
```

- 在Redis Pod外部, 可以通过$(pod name).$(service name).$(namespace).svc.cluster.local方式来访问具体的Pod服务.

### **3.2. 如何理解稳定持久化存储?**

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=MDk5ODM4MTUzMjZiMjJkYTE1YjM5NmQ5NjkxMmI1YzBfNGphMTU0enN4NWIySk52OTJqakc2TTM3c0lMemNzMnZfVG9rZW46Ym94Y25CUXVkOUFZQWJOSFBFamh0VFMwcTlmXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

每个Redis Pod对应一个自己的PVC/PV.当Pod发生调度时,需要在别的节点启动时,根据Pod背后关联的存储信息保证其名称的稳定性.

Pod还是会attach挂载到原来的PV/PVC中,从而确定每个Pod有自己专用的存储卷.

1. **4. 总结**

本文主要介绍了无状态和有状态服务在K8S中的典型应用场景.

通过对Deployment部署无状态服务所遇到问题的分析,引出了Stateful新的部署组件.它是通过支持Pod一些特性(e.g. 名称唯一性,稳定的网络标识, 稳定的持久化存储等)来实现在K8S中部署运维有状态服务.

**牢记: Stateful有状态****服务****,每个****Pod****有独立的PVC/PV存储组件**

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=OGI3NDlkNjIxOTgzZjE5MDk3NWVlNWVmMjRkNmQ0OTBfeGVsZTVNTXRKSU96Wkw5MnZBS1hTTVFkOGVqYTE5ZlJfVG9rZW46Ym94Y25PNGZoajZlNnN6WHc1UTlpZHc0SUxSXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# **PV****和****PVC****的引入**

Pod 通常是由应用的开发人员维护，而 Volume 则通常是由存储系统的管理员维护。开发人员要获得上面的信息：

- 要么询问管理员。
- 要么自己就是管理员。 这样就带来一个管理上的问题：应用开发人员和系统管理员的职责耦合在一起了。如果系统规模较小或者对于开发环境这样的情况还可以接受。但当集群规模变大，特别是对于生成环境，考虑到效率和安全性，这就成了必须要解决的问题。

Kubernetes 给出的解决方案是 PersistentVolume （PV）和 PersistentVolumeClaim（PVC）。

PersistentVolume (PV) 是**外部存储系统中的一块存储空间**，由管理员创建和维护。与 Volume 一样，PV 具有持久性，生命周期独立于 Pod。

PersistentVolumeClaim (PVC) 是对 **PV** **的申请 (Claim)。**PVC 通常由普通用户创建和维护。需要为 Pod 分配存储资源时，用户可以创建一个 PVC，指明存储资源的容量大小和访问模式（比如只读）等信息，Kubernetes 会查找并提供满足条件的 PV。

有了 PersistentVolumeClaim，用户只需要告诉 Kubernetes 需要什么样的存储资源，而不必关心真正的空间从哪里分配，如何访问等底层细节信息。这些 Storage Provider 的底层信息交给管理员来处理，只有管理员才应该关心创建 PersistentVolume 的细节信息。

# **StorageClass**

https://cloud.tencent.com/developer/article/1754329

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmM5Y2ZiMzlkMWUwZjY2NzE3YjA0NWNiNGI2MjczMzJfckdFaERXeFBjWEhHb0RJVUJuOHZXblBSUnBScFJHamJfVG9rZW46Ym94Y25TOGxudThQUnAxa2F3dUdpNmdIVlhiXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# **K8s ingress traefik**

https://zhuanlan.zhihu.com/p/117617642 ingress controter

https://cloud.tencent.com/developer/article/1575566

完成具体工作的组件就是Ingress Controller，Traefik就是其中之一。

https://www.jianshu.com/p/cfe8277c11c1

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGIzNzhhMjE5YjJjZmUyNThhOTg1NTAyNzQ2NmM1NjJfM3dtV3JibnMzT1RXZmlnVkZ3ZG1QcmtHWFhrU0tPa2xfVG9rZW46Ym94Y25Seml2c1NESWZGNFhnU21BNWV5aG1iXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

## **Docker-compose traefik**

[https://cloud.tencent.com/developer/article/1393984#:~:text=docker-compose%20](https://cloud.tencent.com/developer/article/1393984#:~:text=docker-compose )

# k8s job

Kubernetes jobs主要是针对短时和批量的工作负载。它是为了结束而运行的，而不是像deployment、replicasets、replication controllers和DaemonSets等其他对象那样持续运行。

对于Kubernetes Jobs最好的用例实践是：

1.批处理任务： 比如说你想每天运行一次批处理任务，或者在指定日程中运行。它可能是像从存储库或数据库中读取文件那样，将它们分配给一个服务来处理文件。

2.运维/ad-hoc任务： 比如你想要运行一个脚本/代码，该脚本/代码会运行一个数据库清理活动，甚至备份一个Kubernetes集群。

https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/job/

## **关于/var/run/docker.sock**

https://www.cnblogs.com/fundebug/p/6723464.html

# ELK(Elasticsearch,Kibana,Logstash和Filebeat）

https://zhuanlan.zhihu.com/p/439380929

## 1 ELK概念

ELK是Elasticsearch、Logstash、Kibana三大开源框架首字母大写简称。市面上也被称为Elastic Stack。其中Elasticsearch是一个基于Lucene、分布式、通过Restful方式进行交互的近实时搜索平台框架。像类似百度、谷歌这种大数据全文搜索引擎的场景都可以使用Elasticsearch作为底层支持框架，可见Elasticsearch提供的搜索能力确实强大,市面上很多时候我们简称Elasticsearch为es。Logstash是ELK的中央数据流引擎，用于从不同目标（文件/数据存储/MQ）收集的不同格式数据，经过过滤后支持输出到不同目的地（文件/MQ/redis/elasticsearch/kafka等）。Kibana可以将elasticsearch的数据通过友好的页面展示出来，提供实时分析的功能。

通过上面对ELK简单的介绍，我们知道了ELK字面意义包含的每个开源框架的功能。市面上很多开发只要提到ELK能够一致说出它是一个日志分析架构技术栈总称，但实际上ELK不仅仅适用于日志分析，它还可以支持其它任何数据分析和收集的场景，日志分析和收集只是更具有代表性。并非唯一性。我们本教程主要也是围绕通过ELK如何搭建一个生产级的日志分析平台来讲解ELK的使用。

官方网站：[https://www.elastic.co/cn/products/](https://link.zhihu.com/?target=https%3A//www.elastic.co/cn/products/)

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=OGJhMjM1N2JhNGRiMjlkNmUyNmQyODA3NGViMDliNzlfamFpSVRDdlZCcG1ZRW5QUFd6RjZVWXVDSWlib0ExakpfVG9rZW46Ym94Y25UN3FNYm5MeGJ0SVN4R3Q5WFNENnRnXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# OPA

#### **OPA** **解决了什么问题**

## **使用动机**

https://zhuanlan.zhihu.com/p/57208435

在一些项目中，我们希望为用户提供类似集群管理员的访问权限。**但为了确保****基线****的安全性和稳定性，我们不希望授予用户完整的集群管理员权限。**例如：

- 我们允许用户完全访问除`kube-system`之外的所有 namespace，因为我们的基础设施（例如监视和日志记录）部署在`kube-system`中；
- 我们希望强制执行 PodSecurityPolicy，不允许用户以`root`的身份运行容器，也不允许用户直接挂载`hostPath`卷。

**面对这些要求，一种解决方案是通过** **Kubernetes** **RBAC** **和一个自定义操作符实现授权。**其基本思想是让所有必要的权限通过 RBAC RoleBindings 进行授予绑定。并且，除了`kube-system`（通过 operator）之外，我们为每个 namespace 的客户提供了 ClusterRole `admin`。每当我们发现某些东西不能像预期那样工作时，我们就会通过每个 namespace 角色或 ClusterRole 添加其他权限。

**但是，这种方式会出现很多针对特定****用例****的单独规则。从长远发展角度来看，这些规则无法得到很好的维护。**特别是在用户群不断增长的情况下，只要有人检测到与配置不匹配的边缘情况，调整角色不太可行。

综上所述，我们不能选择基于白名单的配置授权，而是需要切换到基于黑名单的模型。因为，我们真正想要的是为客户提供集群管理员访问权限，并限制某些特定权限

https://cloud.tencent.com/developer/article/1755148

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=YjJkZjYxZGQ2ZmJmMGY1N2EwMjQxOTRhYTg1NmMyMGFfVldIeENiaVd6WlR2Q1ZoSkxBUDBvcmNxcGZDb1RoMDRfVG9rZW46Ym94Y254UGZKMWNYNG9TSE0xaHpEVTE1RVNkXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

https://zhuanlan.zhihu.com/p/442190502

在实际的生产环境中很多场景中都需要策略控制，比如：

- 需要策略控制用户是否可登陆服务器或者做一些操作；
- 需要策略控制哪些项目/哪些组件可进行部署；
- 需要策略控制如何访问数据库；
- 需要策略控制哪些资源可部署到 Kubernetes 中；

# Postgresql

https://www.zhihu.com/question/20010554

https://www.runoob.com/postgresql/postgresql-create-database.html

# **客户端pgAdmin4使用**

https://www.jianshu.com/p/e7fe45a10cee

https://blog.csdn.net/dorlolo/article/details/113733310

# [Vault](https://github.com/hashicorp/vault) 

[secrets 管理工具 Vault 的介绍、安装及使用 - 于清乐 - 博客园](https://www.cnblogs.com/kirito-c/p/14327708.html)

[Vault](https://github.com/hashicorp/vault) 是 hashicorp 推出的 secrets 管理、加密即服务与权限管理工具。它的功能简介如下：

1. secrets 管理：支持保存各种自定义信息、自动生成各类密钥，vault 自动生成的密钥还能自动轮转(rotate)
2. 认证方式：支持接入各大云厂商的账号体系（比如阿里云RAM子账号体系）或者 LDAP 等进行身份验证，不需要创建额外的账号体系。
3. 权限管理：通过 policy，可以设定非常细致的 ACL 权限。
4. 密钥引擎：也支持接管各大云厂商的账号体系（比如阿里云RAM子账号体系），实现 ACCESS_KEY/SECRET_KEY 的自动轮转。
5. 支持接入 kubernetes rbac 权限体系，通过 serviceaccount+role 为每个 Pod 单独配置权限。

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=MTdhMDViYTc0ZDA4MDRhZTQwODQ1Nzk4ZTAwZjgyMTFfRnFsY0p1SmxKNGx1MFgyZGFPbFJKcnZmV21MMmZNcm9fVG9rZW46Ym94Y25la0RQaUY1WjA3Wmc0N1cwODNSdXZjXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# CMDB

https://zhuanlan.zhihu.com/p/344252888

# **Jenkins**

https://zhuanlan.zhihu.com/p/157592663

# **Jira**

https://www.jianshu.com/p/8c14b52ce692

# **Gerrit**

git pull --rebase

pull最新版本https://www.jianshu.com/p/14f9eb70e99e

## gerrit再次提交

https://blog.csdn.net/feifeibest168/article/details/103874706

gerrit不能对一个已经merge/abandon的change进行再次提交操作,

**要再次提交需要修改Change-Id**

https://www.runoob.com/git/git-reset.html

git reset --soft HEAD

全局找到被abandon的上一条 然后获取他的changeid 然后 git reset --soft changid

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=MmFhZTI4MjAzMjlkYzJmODA4MzZiMzc4ZTFjZmZmZWVfVE96czlNY2pHOTBGaGN0d3NNcFBnOHZ2TFBLS05FRkRfVG9rZW46Ym94Y25idG5QZXo0c2FzSnBjQkNDZ25JT1NoXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

**[git add 忽略某个文件](https://www.cnblogs.com/lykbk/p/sdsddsdsd3w434343434.html)**

https://zhuanlan.zhihu.com/p/264995020

https://www.cnblogs.com/lykbk/p/sdsddsdsd3w434343434.html#:~:text=%E5%91%BD%E4%BB%A4%EF%BC%9Agit%20update-index%20--assume-unchanged%20FILENAME%20%E8%B7%AF%E5%BE%84%2B%E6%96%87%E4%BB%B6%E5%90%8D,%E8%8B%A5%E4%BB%A5%E5%90%8E%E4%B8%8D%E6%83%B3%E5%BF%BD%E7%95%A5%E8%AF%A5%E6%96%87%E4%BB%B6%E7%9A%84%E4%BF%AE%E6%94%B9%EF%BC%8C%E5%88%99%E8%BE%93%E5%85%A5%E5%91%BD%E4%BB%A4%EF%BC%9Agit%20update-index%20--no-assume-unchanged%20FILENAME%20%E5%85%B3%E4%BA%8E%E5%91%BD%E4%BB%A4%EF%BC%9Ahttp%3A%2F%2Fblog.sina.com.cn%2Fs%2Fblog_7d3fd13c0101a4i8.html

### gerrit 的 Change-Id 机制:

首先要明确, Change-Id 是 gerrit (代码审核平台)的概念, 与 git (版本管理) 是没有关系的. 简单来说, Change-Id 是 gerrit 用以追踪具体提交的机制. 这里不贴网上已有的解释,举两个栗子大家体会下:

1. 你已经用 git push 将代码提交 gerrit 审核了,这时你发现代码中有疏漏,修改了一下,执行 git commit --amend, 再次推送还可以成功. 这就是因为 gerrit 检查到两次 push 的 commit 有同一个 change-id, 就认为是同一个提交,因此可以 amend.
2. git push 将代码提交到 gerrit 审核,到 gerrit 网站一看,大红字标着 Can Not Merge 字样. 我想常用 gerrit 的同学肯定都遇到过这问题. 之前我的做法是, git reset 后,更新代码,再重新提交. 现在的做法是,不用 git reset 了,直接 git commit --amend, 删掉 commit log 中的 change-id 那行,然后wq保存退出.这时 gerrit 的那个钩子脚本会再生成一个不同的 change-id ,这时再更新代码,重新提交即可成功. 这里只简要介绍该方法,具体步骤将在 代码冲突 场景中详解. Change-Id 的生成机制请继续向下看.

链接：https://www.jianshu.com/p/9fbc50742b54

### git cherry-pick 命令

https://ruanyifeng.com/blog/2020/04/git-cherry-pick.html

## Git

https://marklodato.github.io/visual-git-guide/index-zh-cn.html

https://blog.csdn.net/qq_40732336/article/details/111182622

https://blog.csdn.net/chuyouyinghe/article/details/124024459

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=ODk3MTA1MTJlMTRlMThlODI4MjcwZGRkZDNmZDA5NjJfdEd5bDV6WVRGS2g5Z2kweko2ajUxMEFrRDA5bDFFTlFfVG9rZW46Ym94Y25FQVhmdE1ZdUhZU3FyTUR0QlFPZmlnXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

Q:如何git add. 如何避免提交一些生产环境不需要的？

# RBAC（Role-Based Access Control）

https://zhuanlan.zhihu.com/p/513142061

# supervisor 

`Supervisor` 是一个客户端/[服务器](https://cloud.tencent.com/product/cvm?from=20065&from_column=20065)系统，允许其用户监视和控制类似`UNIX`的操作系统上的多个进程。

`Supervisor` 是用 `Python` 开发的一套通用的进程管理程序，能将一个普通的命令行进程变为后台`daemon`，并监控进程状态，异常退出时能自动重启。

它是通过`fork/exec`的方式把这些被管理的进程当作`supervisor`的子进程来启动，这样只要在`supervisor`的配置文件中，把要管理的进程的可执行文件的路径写进去即可。也实现当子进程挂掉的时候，父进程可以准确获取子进程挂掉的信息的，可以选择是否自己启动和报警。`supervisor`还提供了一个功能，可以为`supervisord`或者每个子进程，设置一个非`root`的`user`，这个`user`就可以管理它对应的进程。

https://cloud.tencent.com/developer/article/1665596

### 为什么要加nginx -g "daemon off;"

1. 容器只对主进程服务，当主进程退出的时候，容器也会退出。
2. CMD在使用 shell 格式的话，真实的命令会被包装为 sh -c 的参数的形式进行执行的。例如，启用nginx的例子（当然这个命令是错误的）：

```Bash
CMD service nginx start
```

该命令会被包装为：

```Bash
CMD [ "sh", "-c", "service nginx start"]
```

# 总结

结合上面两点，可以看出，启动nginx时，主进程其实是sh，而容器是因为sh这个主进程产生的，当该条命令执行完毕之后，sh作为主进程会退出，容器自然也会退出，因此导致nginx启动不了。所以需要使用

CMD ["nginx", "-g", "daemon off;"] 启动

# Python2和Python3的区别

https://zhuanlan.zhihu.com/p/161380701

# Request Header 和 Response Header字段详解

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=OTY5ZGQzOGJiODA1ZWYzZjczODc5MWJiOWIyMmRkNzFfeUFVVTBmNUlpaEZTRjVFMXNQTGxhZkdrOW9OenlxMVRfVG9rZW46Ym94Y241cDNxbkJKZ2NwNlE4RzcwRVNSMWpiXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# Docker安全机制详解(容器资源限制)

https://blog.csdn.net/ly2020_/article/details/98845461

# OpenStack

http://www.chenshake.com/?s=openstack&searchsubmit=

# Kafka

https://cloud.tencent.com/developer/article/1547380

# 统一网关之Gateway

https://zhuanlan.zhihu.com/p/577917806

![img](https://nio.feishu.cn/space/api/box/stream/download/asynccode/?code=ODZkOWI4ZGE5ZTA4Y2YzNTIxNmVjNTc3N2VjNGVkODlfbkJCMllvclJ6dDA4WnpURzltQmxQblBsOElCbDdmeENfVG9rZW46Ym94Y25mN1JaQ1FpZGRDZ3N5aWxTQjEzcERiXzE2ODEwOTQ1MjE6MTY4MTA5ODEyMV9WNA)

# 蓝绿部署、滚动部署、灰度发布、金丝雀发布

https://cloud.tencent.com/developer/article/1449209

## `金丝雀发布`

只准备几台服务器，在上面部署新版本的系统并测试验证。测试通过之后，担心出现意外，还不敢立即更新所有的服务器。 先将线上的一万台服务器中的10台更新为最新的系统，然后观察验证。确认没有异常之后，再将剩余的所有服务器更新。

这个方法就是`金丝雀发布`

## **蓝绿部署**

蓝绿部署的目的是`减少发布时的中断时间`、

蓝绿部署中，一共有两套系统：一套是正在提供服务系统，标记为“绿色”；另一套是准备发布的系统，标记为“蓝色”。两套系统都是功能完善的，并且正在运行的系统，只是系统版本和对外服务情况不同。

# Dockerfile 的 RUN 和 CMD、ENTRYPOINT

https://cloud.tencent.com/developer/article/1795329

印象最深的的问题以及解决

持续补充学习笔记中......