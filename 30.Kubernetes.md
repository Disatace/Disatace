# dockerfile、docker compose、k8s区别

- dockerfile: 构建单个服务镜像，以脚本形式
- docker-compose：多镜像编排容器
- k8s：跨服务编排

### 几种技术的应用场景及资源占用情况

| 技术           | 应用场景              | 资源占用情况 |
| :------------- | :-------------------- | :----------- |
| Docker         | 单机部署简单应用      | 低           |
| Docker-Compose | 单机/少数机器部署应用 | 低           |
| k8s            | 集群部署高可用应用    | 高           |

# Kubernetes 是什么？

[徐亚松的博客|云原生|容器|python|go|监控|k8s|docker|分布式|文章|分享 | 博客 | (xuyasong.com)](http://www.xuyasong.com/)

Kubernetes是一个可移植的、可扩展的开源平台，用于**管理容器化的工作负载和服务**，可促进声明式配 置和自动化。kubernetes拥有一个庞大且快速增长的生态系统。kubernetes的服务、支持和工具广泛可 用。 

kubernetes这个名字源于希腊于，意为舵手或飞行员。k8s这个缩写是因为k和s之间有八个字符的关 系。google在2014年开源了kubernetes项目。kubernetes建立在google在大规模运行生产工作负载方 面拥有十几年的经验的基础上，结合了社区中最好的想法和实践。

# 时光回溯



![image-20230303142719198](http://cdn.gtrinee.top/image-20230303142719198.png)

#### 传统部署时代 

早期，各个组织机构在物理服务器上运行应用程序。无法为服务器中的应用程序定义资源边界，这会导 致资源分配问题。例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部 分资源的情况，结果可能导致其他应用程序的性能下降。一种解决方案是在不同的物理服务器上运行每 个应用程序，但是由于资源利用不足而服务扩展，并且维护许多物理服务器的**成本很高** 

#### 虚拟化部署时代 

作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的cpu上允许多个虚拟机。虚拟化 允许应用程序在虚拟机之间隔离，并提供一定程度的安全，因为一个应用程序的信息不能被另一应用程 序随意访问。 **虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序而可以实现更 好地可伸缩性，降低硬件成本等等**。 每个VM是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括自己的操作系统。 

#### 容器部署时代：

 容器类似于VM，但是他们具有被放宽的隔离属性，可以在应用程序之间**共享操作系统（OS）。因此， 容器被认为是轻量级的。**容器与VM类似，具有自己的文件系统、cpu、内存、进程空间等。由于他们与 基础架构分离，因此可以跨云和os发行版本进行移植。 容器因具有许多优势而变得流行起来，下面列出的是容器的一些好处：

```
敏捷应用程序的创建和部署：与使用vm镜像相比，提高了容器镜像创建的简便性和效率。

持续开发、继承和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的容器镜像
构建和部署。

关注开发与运维的分离：在构建、发布时而不是在部署时创建应用程序容器镜像，从而将应用程序
与基础架构分离。

可观察性、测试和生产的环境一致性：在便携式计算机上与在云中相同的运行。

跨云和操作系统发行版本地可移植性：可在Ubuntu、RHEL、CoreOS、本地、Google Kubernets
Engine和其他任何地方运行

以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行os到使用逻辑资源在os上运行应用
程序。

松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署
和管理-而不是在一台大型单机上整体运行。

资源隔离：可预测的应用程序性能。

资源利用：高效率和高密度。
```

### 但是纯容器模式也会出现一些问题

1. 业务容器数量庞大，哪些容器部署在哪些节点，使用了哪些端口，如何记录、管理，需要登录到每台机器去管理？
2. 跨主机通信，多个机器中的容器之间相互调用如何做，iptables规则手动维护？
3. 跨主机容器间互相调用，配置如何写？写死固定IP+端口？
4. 如何实现业务高可用？多个容器对外提供服务如何实现负载均衡？
5. 容器的业务中断了，如何可以感知到，感知到以后，如何自动启动新的容器?
6. 如何实现滚动升级保证业务的连续性？
7. ......

# 所以我们需要使用kubernetes

容器时打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停 机。例如，如果一个容器发生故障，则需要启动另一个容器。如果系统如理此问题，会不会更容易？ 

这就是kubernetes来解决这些问题的方法！Kubernetes为你提供了一个**可弹性运行分布式系统的框 架。**Kubernetes会满足你的扩展要求、故障转移、部署模式等。 

kubernetes为你提供：

- 自动装箱 

  建构于容器之上，基于**资源依赖及其他约束自动完成容器部署且不影响其可用性**，并通过调度机制混合 关键性应用和非关键型应用的工作负载于同一节点以提升资源利用率 

- 自我修复（自愈） 

  支持容器故障后自动重启、节点故障后重新调度容器，以及其他可用节点、健康状态检查失败后关闭容 器并重新创建等自我修复机制。 

- 水平扩展 

  支持通过**命令或UI手动水平扩展**，以及基于cpu等资源负载率的自动水平扩展机制。 

- 服务发现和负载均衡 

  Kubernets通过其附加组件KubeDNS（或CoreDNS）为系统内置了服务发现功能，它会为每个service 配置DNS名称，并允许集群内的客户端直接使用此名称发出请求，**而Service则通过iptables或ipvs内建 了负载均衡机制。**

- 自动发布和回滚 

  Kubernets支持**灰度更新应用程序或其配置信息**，它会监控更新过程中应用程序的健康状态，以确保它 不会在同一时刻杀掉所有实例，而此过程中**一旦有故障发生，就会立即自动执行回滚操作** 

- 密钥和配置管理 

  kubernetsd的configmap实现了配置数据与Docker镜像解耦，需要时，仅对配置做出变更而无须重新构 建docker镜像，这为应用开发部署带来了很大的灵活性。此外，对于应用所依赖的一些敏感数据，如用 户名和密码、令牌、密钥等信息，Kubernetes专门提供了secret对象为其解耦，既便利了应用的快速开 发和交付，又提供一定程度上的安全保障。 

- 存储编排 

  Kubernets支持pod对象**按需自动挂载不同类型的存储系统，**这包括节点本地存储、公有云服务商的云 存储，以及网络存储系统（例如，NFS/ISCSI/GlusterFS/Ceph/Cinder/Flocker等）

#### 架构图

区分组件与资源

![在这里插入图片描述](https://img-blog.csdnimg.cn/b7899a4753d646239b93ccf7de8a3698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ3ODU1NDYz,size_16,color_FFFFFF,t_70)

<img src="https://static.sitestack.cn/projects/itmuch-kubernetes/kubernetes/02-Kubernetes%20Architecture/images/k8s-architecture.png" alt="11-K8s架构及基本概念 - 《周立 Kubernetes 开源书》 - 书栈网 · BookStack"  />



#### 核心组件

- **ETCD：**分布式**高性能键值数据库**,**存储**整个集群的所有元数据   etcd是独立的服务组件，并不隶属于Kubernetes集群自身。

  etcd不仅能够提供键值数据存储，而且还为其提供了监听机制，用于监听和推送变更。Kubernetes集群 系统中，etcd中的键值发生变化时会通知到api server。

- **kube-apiserver:**  API服务器,集群资源访问控制入口,提供restAPI及安全访问控制

  Api server负责输出restful风格的kubernetes API，它是发往集群的所有REST操作命令的接入点，并负 责接收、校验并响应所有的		REST请求，结果状态被**持久存储于etcd中**。因此，api server是整个集群的 网关。

- **Scheduler：**调度器,负责把业务容器调度到最合适的Node节点
- **Controller Manager：**控制器管理,确保集群资源按照期望的方式运行   ，由控制器完成的功能主要包括生命周期功能和api业务逻辑。
  - Replication Controller
  - Node controller
  - ResourceQuota Controller
  - Namespace Controller
  - ServiceAccount Controller
  - Tocken Controller
  - Service Controller
  - Endpoints Controller

**Node组件**

- **kubelet：**运行在每运行在每个节点上的主要的“节点代理”个节点上的主要的“节点代理”
  - pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。
  - 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理.
  - 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要
- **kubectl:** 命令行接口，用于对 Kubernetes 集群运行命令  https://kubernetes.io/zh/docs/reference/kubectl/ 
- **kubeproxy：**每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为service资源对象生成iptables或者 ipvs规则，从而捕获当前service的流量并将其转发至正确的后端pod对象。
- CNI实现: 通用网络接口, 我们使用flannel来作为k8s集群的网络插件, 实现跨节点通信

![image-20230303165130574](http://cdn.gtrinee.top/image-20230303165130574.png)

## 组件

一个kubernetes集群由一组被称作节点的机器组成。这些节点上运行Kubernetes所管理的容器化应 用。集群具有至少一个工作节点。 

工作节点托管作为应用负载的组件的Pod。控制平面管理集群中的工作节点和pod。为集群提供故障转 移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。 

**Mater：**集群中的一台服务器用作Master，负责管理整个集群。Master是集群的网关和中枢，负责诸如 为用户和客户端暴露API、跟踪其他服务器的健康状态、以最优方式调度工作负载，以及编排其他组件之 间的通信任务，它是用户或客户端与集群之间的核心联络点，并负载Kubernetes系统的大多数集中式管 控逻辑。单个master节点即可完成其所有的功能，但出于冗余及负载均衡等目的，生产环境中通常需要 协同部署多个此类主机。 

**Node：**Node是Kubernetes集群的工作节点，负责接收来 自master的工作指令相应地创建或销毁Pod 对象，以及调整网络规则以合理地路由和转发流量等。理论上讲，Node可以是任何形式的计算设备，不 过Master会统一将其抽象为Node对象进行管理。 

Kubernetes将所有Node资源集结于一处形成一台更加强大的服务器，在用户将应用部署于其上时， Master会使用调度算法将其自动指派至某个特定的Node运行。在Node加入集群或从集群中移除时， Master也会按需重新编排影响到的Pod，于是用户无须关心其应用究竟运行何处。



## 工作流程

![](http://cdn.gtrinee.top/process.png)

1. 用户准备一个资源文件（记录了业务应用的名称、镜像地址等信息），通过调用APIServer执行创建Pod
2. APIServer收到用户的Pod创建请求，将Pod信息写入到etcd中
3. 调度器通过list-watch的方式，发现有新的pod数据，但是这个pod还没有绑定到某一个节点中
4. 调度器通过调度算法，计算出最适合该pod运行的节点，并调用APIServer，把信息更新到etcd中
5. kubelet同样通过list-watch方式，发现有新的pod调度到本机的节点了，因此调用容器运行时，去根据pod的描述信息，拉取镜像，启动容器，同时生成事件信息
6. 同时，把容器的信息、事件及状态也通过APIServer写入到etcd中、



# k8s集群安装

```
master k8s-master1 
node k8s-node1 
node k8s-node2
```



![image-20230303222504179](http://cdn.gtrinee.top/image-20230303222504179.png)







![image-20230303222359054](http://cdn.gtrinee.top/image-20230303222359054.png)

## 安装前准备工作

### 1. 设置hosts解析

操作节点：所有节点（`k8s-master，k8s-slave`）均需执行

- **修改hostname**
  hostname必须只能包含小写字母、数字、","、"-"，且开头结尾必须是小写字母或数字

``` python
# 在master节点
$ hostnamectl set-hostname k8s-master #设置master节点的hostname

# 在slave-1节点
$ hostnamectl set-hostname k8s-slave1 #设置slave1节点的hostname

# 在slave-2节点
$ hostnamectl set-hostname k8s-slave2 #设置slave2节点的hostname
```

- **添加hosts解析**

``` python
$ cat >>/etc/hosts<<EOF
172.21.32.15 k8s-master
172.21.32.11 k8s-slave1
172.21.32.9 k8s-slave2
EOF
```

### 2. 调整系统配置

操作节点： 所有的master和slave节点（`k8s-master,k8s-slave`）需要执行

>本章下述操作均以k8s-master为例，其他节点均是相同的操作（ip和hostname的值换成对应机器的真实值）

- **设置安全组开放端口**

如果节点间无安全组限制（内网机器间可以任意访问），可以忽略，否则，至少保证如下端口可通：
k8s-master节点：TCP：6443，2379，2380，60080，60081UDP协议端口全部打开
k8s-slave节点：UDP协议端口全部打开

- **设置iptables**

``` python
iptables -P FORWARD ACCEPT
```

- **关闭swap**

``` python
swapoff -a
# 防止开机自动挂载 swap 分区
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

- **关闭selinux和防火墙**

``` python
sed -ri 's#(SELINUX=).*#\1disabled#' /etc/selinux/config
setenforce 0
systemctl disable firewalld && systemctl stop firewalld
```

- **修改内核参数**

``` python
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward=1
vm.max_map_count=262144
EOF
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
```

- 设置yum源

```powershell
$ curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo
$ curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
$ yum clean all && yum makecache
```

### 3. 安装docker

操作节点： 所有节点

``` python
 ## 查看所有的可用版本
$ yum list docker-ce --showduplicates | sort -r

## 安装源里最新版本
$ yum install docker-ce

## 配置docker加速
$ mkdir -p /etc/docker
vi /etc/docker/daemon.json
{
  "registry-mirrors": ["https://v117q3rj.mirror.aliyuncs.com"]
}

## 启动docker
$ systemctl enable docker && systemctl start docker
```



## 部署kubernetes

### 1. 安装 kubeadm, kubelet 和 kubectl

**！！！！！注意版本安装一点安对 否则初始化会不成功！**

操作节点： 所有的master和slave节点(`k8s-master,k8s-slave`) 需要执行

``` powershell
$ yum install -y kubelet-1.16.2 kubeadm-1.16.2 kubectl-1.16.2 --disableexcludes=kubernetes
## 查看kubeadm 版本
$ kubeadm version
## 设置kubelet开机启动
$ systemctl enable kubelet 
```

### 2. 初始化配置文件

操作节点： 只在master节点（`k8s-master`）执行

``` yaml
$ kubeadm config print init-defaults > kubeadm.yaml
$ cat kubeadm.yaml
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.153.50 # apiserver地址，因为单master，所以配置master的节点内网IP
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers  # 修改成阿里镜像源
kind: ClusterConfiguration
kubernetesVersion: v1.16.2
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16  # Pod 网段，flannel插件需要使用这个网段
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

>  对于上面的资源清单的文档比较杂，要想完整了解上面的资源对象对应的属性，可以查看对应的 godoc 文档，地址: https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2。 

### 3. 提前下载镜像

操作节点：只在master节点（`k8s-master`）执行

``` python
  # 查看需要使用的镜像列表,若无问题，将得到如下列表
$ kubeadm config images list --config kubeadm.yaml
registry.aliyuncs.com/google_containers/kube-apiserver:v1.16.0
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.16.0
registry.aliyuncs.com/google_containers/kube-scheduler:v1.16.0
registry.aliyuncs.com/google_containers/kube-proxy:v1.16.0
registry.aliyuncs.com/google_containers/pause:3.1
registry.aliyuncs.com/google_containers/etcd:3.3.15-0
registry.aliyuncs.com/google_containers/coredns:1.6.2
  # 提前下载镜像到本地
$ kubeadm config images pull --config kubeadm.yaml
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.16.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.16.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.16.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.16.0
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.1
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.3.15-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:1.6.2
```

重要更新：如果出现不可用的情况，可以分开手动pull

### 4. 初始化master节点

操作节点：只在master节点（`k8s-master`）执行

``` python
kubeadm init --config kubeadm.yaml
```

若初始化成功后，最后会提示如下信息：

```bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.153.50:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:eebe030ededf7e969828a0fa26e8512b39673355e817a4b102eb179
```

接下来按照上述提示信息操作，配置kubectl客户端的认证

``` python
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

> **⚠️注意：**此时使用 kubectl get nodes查看节点应该处于notReady状态，因为还未配置网络插件
>
> 若执行初始化过程中出错，根据错误信息调整后，执行kubeadm reset后再次执行init操作即可

### 5. 添加slave节点到集群中

操作节点：所有的slave节点（`k8s-slave`）需要执行
在每台slave节点，执行如下命令，该命令是在kubeadm init成功后提示信息中打印出来的，需要替换成实际init后打印出的命令。

!如果出现下图情况 是因为token过期

```bash
此时需要通过kubedam重新生成token

[root@master ~]#kubeadm token generate #生成token
7r3l16.5yzfksso5ty2zzie #下面这条命令中会用到该结果
[root@master ~]# kubeadm token create 7r3l16.5yzfksso5ty2zzie  --print-join-command --ttl=0  #根据token输出添加命令
W0604 10:35:00.523781   14568 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0604 10:35:00.523827   14568 validation.go:28] Cannot validate kubelet config - no validator is available
kubeadm join 192.168.254.100:6443 --token 7r3l16.5yzfksso5ty2zzie     --discovery-token-ca-cert-hash sha256:56281a8be26

```

![image-20230307001418516](http://cdn.gtrinee.top/image-20230307001418516.png)

``` python
kubeadm join 172.21.32.15:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:1c4305f032f4bf534f628c32f5039084f4b103c922ff71b12a5f0f98d1ca9a4f
```

### 6. 安装flannel插件

### 

操作节点：只在master节点（`k8s-master`）执行

- 下载flannel的yaml文件

```powershell
wget https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
```

- 修改配置，指定网卡名称，大概在文件的和170行，190行，添加一行配置：

```powershell
$ vi kube-flannel.yml
...      
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth0  # 如果机器存在多网卡的话，指定内网网卡的名称，默认不指定的话会找第一块网
        resources:
          requests:
            cpu: "100m"
...
```

- 执行安装flannel网络插件

```powershell
# 先拉取镜像,此过程国内速度比较慢
$ docker pull quay.io/coreos/flannel:v0.11.0-amd64
# 执行flannel安装
$ kubectl create -f kube-flannel.yml
```

### 7.  设置master节点是否可调度（可选）

操作节点：`k8s-master`

默认部署成功后，master节点无法调度业务pod，如需设置master节点也可以参与pod的调度，需执行：

``` python
$ kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule-
```

### 8. 验证集群

操作节点： 在master节点（`k8s-master`）执行

```bash

[root@master install]# kubectl get nodes
NAME     STATUS     ROLES    AGE     VERSION
master   NotReady   master   70m     v1.16.2
node1    NotReady   <none>   4m58s   v1.16.2
node2    NotReady   <none>   4m52s   v1.16.2

```

出现状态为notready

master一直处于NotReady状态查看日志出现 failed to find plugin “flannel” in path [/opt/cni/bin]

> **日志信息**

![image-20230307003207353](http://cdn.gtrinee.top/image-20230307003207353.png)

查看 /opt/cni/bin 缺少 flannel 

### 解决方案

需要下载CNI插件：CNI plugins v0.8.6

github下载地址：https://github.com/containernetworking/plugins/releases/tag/v0.8.6

(在1.0.0版本后CNI Plugins中没有flannel)

在windows下载后用mobaxtem传到linux上

下载后通过xftp 上传到Linux /home/install目录解压


```bash
tar zxvf cni-plugins-linux-amd64-v0.8.6.tgz
```

 复制 flannel 到 /opt/cni/bin/

```cobol
[root@k8s-node1 home]# cp flannel /opt/cni/bin/
```

查看节点状态

依旧没有ready发现还有其他问题

![image-20230307124134024](http://cdn.gtrinee.top/image-20230307124134024.png)

可以观察到提示 failed to get cgroup stats for “/system.slice/docker.service” 错误，下面是分析与解决该问题的过程

参考[(17条消息) 解决 Kubernetes 中 Kubelet 组件报 failed to get cgroup Failed to get system container stats 错误-CSDN博客](https://blog.csdn.net/shelutai/article/details/122665147)

编辑 /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 文件，并添加下面配置：

CPUAccounting=true
MemoryAccounting=true

具体操作如下：

```bash
$ vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf


[Service]
CPUAccounting=true              ## 添加 CPUAccounting=true 选项，开启 systemd CPU 统计功能
MemoryAccounting=true           ## 添加 MemoryAccounting=true 选项，开启 systemd Memory 统计功能
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```



```BASH
2、重启 Kubelet 服务
重启 kubelet 服务，让 kubelet 重新加载配置。

$ systemctl daemon-reload
$ systemctl restart kubelet
BASH
3、观察 kubelet 日志
重启完 kubelet 后等一段时间，再次观察 kubelet 日志信息：

```

发现master 已经ready 但是node没有

![image-20230307124042010](http://cdn.gtrinee.top/image-20230307124042010.png)

同样的将flanne  scp到node机器上

```bash

[root@master install]# scp flannel root@192.168.153.60:/opt/cni/bin/
The authenticity of host '192.168.153.60 (192.168.153.60)' can't be established.
ECDSA key fingerprint is SHA256:bbiQ4gkrxf/77ivOQDKsG1TfwQOc1Pc0OgRYcv+wSzo.
ECDSA key fingerprint is MD5:fd:65:03:69:64:95:29:46:36:92:c6:31:e5:4b:45:98.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.153.60' (ECDSA) to the list of known hosts.
root@192.168.153.60's password:
flannel                                                       
```

然后restart  kubelet

再看 均ready

![image-20230307124400507](http://cdn.gtrinee.top/image-20230307124400507.png)





## 验证集群

创建测试nginx服务

```bash
[root@master install]# kubectl run test-nginx --image=nginx
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/test-nginx created

#查看pod是否创建成功，并访问pod ip测试是否可用
[root@master install]# kubectl get deployment
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
test-nginx   1/1     1            1           3m52s
[root@master install]# kubectl get po -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP           NODE    NOMINATED NODE   READINESS GATES
test-nginx-855cd5d5c9-xcvfs   1/1     Running   0          4m11s   10.244.1.2   node1   <none>           <none>
[root@master install]# curl 10.244.1.2
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@master install]#


```

### 9. 部署dashboard

- 部署服务

```powershell
# 推荐使用下面这种方式
$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/recommended.yaml
$ vi recommended.yaml
# 修改Service为NodePort类型
......
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
  type: NodePort  # 加上type=NodePort变成NodePort类型的服务
......
```

- 查看访问地址，本例为30133端口

```powershell
kubectl create -f recommended.yaml
kubectl -n kubernetes-dashboard get svc
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.105.62.124   <none>        8000/TCP        31m
kubernetes-dashboard        NodePort    10.103.74.46    <none>        443:30133/TCP   31m 
```

- 使用浏览器访问 https://62.234.133.177:30133，其中62.234.133.177为master节点的外网ip地址，chrome目前由于安全限制，测试访问不了，使用firefox可以进行访问。

![image-20230307130854689](http://cdn.gtrinee.top/image-20230307130854689.png)

![image-20230307131419664](http://cdn.gtrinee.top/image-20230307131419664.png)

- 创建ServiceAccount进行访问

```powershell
$ vi admin.conf
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: admin
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kubernetes-dashboard

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kubernetes-dashboard

$ kubectl create -f admin.conf
$ kubectl -n kubernetes-dashboard get secret |grep admin-token
admin-token-fqdpf                  kubernetes.io/service-account-token   3      7m17s
# 使用该命令拿到token，然后粘贴到
$ kubectl -n kubernetes-dashboard get secret admin-token-fqdpf -o jsonpath={.data.token}|base64 -d
eyJhbGciOiJSUzI1NiIsImtpZCI6Ik1rb2xHWHMwbWFPMjJaRzhleGRqaExnVi1BLVNRc2txaEhETmVpRzlDeDQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1mcWRwZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjYyNWMxNjJlLTQ1ZG...

```



![](F:\BaiduNetdiskDownload\14小时搞定k8s企业级devops实践\课件\DevOps训练营课件最新版(2020-04-11)\images\dashboard.png)

![image-20230307132002662](http://cdn.gtrinee.top/image-20230307132002662.png)





# 资源对象

kubernetes系统的 a**pi server基于http/https接收并响应客户端的操作请求**，它提供了一种基于资源的 RESTful风格的编程结构，将集群的各种组件都抽象成为标准的REST资源，如Node、Namespace和Pod 等，**并支持通过标准的HTTP方法以JSON为数据序列化方案**进行资源管理操作。 

kubernetes系统将一切事物都抽象为API资源

## 工作负载型资

Pod是工作负载型资源中的基础资源，它**负责运行容器**，并为其解决环境性的依赖。但pod可能会因为 资源超限或节点故障等原因而终止，这些非正常终止的Pod资源需要被重建，不过这类工作将由工作负 载型的控制器来完成，他们通常也称之为pod控制器。

```
ReplicaSet：用于确保每个pod副本在任意时刻均能满足目标数量，换言之，它用于保证每个容器
或容器组总是运行并且可访问。

Deployment：它用于为pod和ReplicaSet提供声明式更新，是建构在ReplicaSet之上的更为高级的控制器

StatefulSet：用于管理有状态的持久化应用，如database服务程序；其与Deployment的不同之处
在于StatefulSet会为每一个Pod创建一个独有的持久性标识符，并会确保各pod之间的顺序

DaemonSet：用于确保每个节点都运行了某pod的一个副本，新增的节点一样会被添加此类pod；在节点被移除时，此类pod会被回收

Job：用于管理运行完成后即可终止的应用，例如批处理作业任务
```

# 配置与存储 

Docker容器分层联合挂载的方式决定了不宜在容器内部存储需要持久化的数据，于是它通过引入挂载外 部存储卷的方式来解决此类问题，**而kubernetes则为此涉及了volume资源**，它支持众多类型的存储设 备或存储系统。

 另外，基于镜像构建容器应用时，其配置信息与镜像制作时写入，从而为不同的环境定制变得困难， Docker使用环境变量等作为解决方案，但这么一来就得于容器启动时将值传入，且无法在运行时修改。 configMap资源能够以环境变量或存储卷的方式接入到pod资源的容器中，并且可被多个同类的pod资源 共享引用，从而实现“一次修改，多处生效”。不过，这种方式不适于存储敏感数据，如私钥、密码等， 那就是另一个资源类型Secret的功能。 

## 集群级资源

kubernetes还存在一些集群级别的资源，用于定义集群自身配置信息的对象，他们仅仅应该由集群管理 员进行操作。 

- Namespace：资源对象名称的作用范围，绝大多数对象都隶属于某个名称空间，默认时隶属于 defalut 

- Node：Kubernetes集群的工作节点，其标识符在当前集群中必须是唯一的。 

- Role：名称空间级别的由规则组成的权限集合 

- ClusterRole、RoleBinding、ClusterRoleBinding......

# 资源管理 

## 资源管理介绍

在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。

> kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其 实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。
>
>  kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在 Pod 中，而kubernetes一般 也不会直接管理Pod，而是通过 Pod控制器 来管理Pod的。 
>
> Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了 Service 资源实现这 个功能。 当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种 存储 系统。

![image-20230307200847165](http://cdn.gtrinee.top/image-20230307200847165.png)





## 资源管理方式

```bash
命令式对象管理：直接使用命令去操作kubernetes资源
kubectl run nginx-pod --image=nginx:1.17.1 --port=80
命令式对象配置：通过命令配置和配置文件去操作kubernetes资源
kubectl create/patch -f nginx-pod.yaml
声明式对象配置：通过apply命令和配置文件去操作kubernetes资源
kubectl apply -f nginx-pod.yaml
```

### 命令式对象管理 

# kubectl命令 

[Kubectl 常用命令大全 - 腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1638810)



kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器 化应用的安装部署。kubectl命令的语法如下：

```bash
kubectl [command] [type] [name] [flags]

comand：指定要对资源执行的操作，例如create、get、delete
type：指定资源类型，比如deployment、pod、service
name：指定资源的名称，名称大小写敏感
flags：指定额外的可选参数

# 查看所有pod
kubectl get pod
# 查看某个pod
kubectl get pod pod_name
# 查看某个pod,以yaml格式展示结果
kubectl get pod pod_name -o yaml


#kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看:
kubectl api-resources
```

![image-20230309133531464](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20230309133531464.png)



下面以一个namespace / pod的创建和删除简单演示下命令的使用：

```bash
# 创建一个namespace
[root@master ~]# kubectl create namespace dev
namespace/dev created
# 获取namespace
[root@master ~]# kubectl get ns
NAME STATUS AGE
default Active 21h
dev Active 21s
kube-node-lease Active 21h
kube-public Active 21h
kube-system Active 21h

# 在此namespace下创建并运行一个nginx的Pod
[root@master ~]# kubectl run pod --image=nginx:latest -n dev
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed
in a future version. Use kubectl run --generator=run-pod/v1 or kubectl
create instead.
deployment.apps/pod created

# 查看新创建的pod
[root@master ~]# kubectl get pod -n dev
NAME READY STATUS RESTARTS AGE
pod 1/1 Running 0 21s

# 删除指定的pod
[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x
pod "pod" deleted

# 删除指定的namespace
[root@master ~]# kubectl delete ns dev
namespace "dev" deleted

```



### 命令式对象配置 

命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。 

1） 创建一个nginxpod.yaml，内容如下：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev

---
apiVersion: v1
kind: Pod
metadata:
  name: nginxpod
  namespace: dev
spec:
  containers:
  - name: nginx-containers
    image: nginx:latest

```

2）执行create命令，创建资源：

```bash
[root@master ~]# kubectl create -f nginxpod.yaml
namespace/dev created
pod/nginxpod created
```



 此时发现创建了两个资源对象，分别是namespace和pod 

3）执行get命令，查看资源：

```bash
[root@master ~]# kubectl get -f nginxpod.yaml
NAME STATUS AGE
namespace/dev Active 18s
NAME READY STATUS RESTARTS AGE
pod/nginxpod 1/1 Running 0 17s
```

这样就显示了两个资源对象的信息 

4）执行delete命令，删除资源：

```bash
[root@master ~]# kubectl delete -f nginxpod.yaml
namespace "dev" deleted
pod "nginxpod" deleted

```

```
总结:
命令式对象配置的方式操作资源，可以简单的认为：命令 + yaml配置文件（里面是命令需要的
各种参数）
```

### 声明式对象配置

声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。

```bash
# 首先执行一次kubectl apply -f yaml文件，发现创建了资源
[root@master ~]# kubectl apply -f nginxpod.yaml
namespace/dev created
pod/nginxpod created

# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动
[root@master ~]# kubectl apply -f nginxpod.yaml
namespace/dev unchanged
pod/nginxpod unchanged


总结:
其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）
使用apply操作资源：
如果资源不存在，就创建，相当于 kubectl create
如果资源已存在，就更新，相当于 kubectl patch

```



**如果想在node节点运行kubectl需要配置文件**

kubectl的运行是需要进行配置的，它的配置文件是$HOME/.kube，如果想要在node节点运行此命令， 需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作：

```
scp -r HOME/.kube node1: HOME/
```

# Namespace

Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现**多套环境的资源隔离 或者多租户的资源隔离**。 

默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个 Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集 群内部的资源分配到不同的Namespace中，可以形成逻辑上的"组"，**以方便不同的组的资源进行隔离使 用和管理。** 

可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户 的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用 量、内存使用量等等，来实现租户可用资源的管理。





```bash
#kubernetes在集群启动之后，会默认创建几个namespace

[root@master ~]# kubectl get namespace
NAME STATUS AGE
default Active 45h # 所有未指定Namespace的对象都会被分配在default
命名空间
kube-node-lease Active 45h # 集群节点之间的心跳维护，v1.13开始引入
kube-public Active 45h # 此命名空间下的资源可以被所有人访问（包括未认证
用户）
kube-system Active 45h # 所有由Kubernetes系统创建的资源都处于这个命名
空间


# 2 查看指定的ns 命令：kubectl get ns ns名称
[root@master ~]# kubectl get ns default
NAME STATUS AGE
default Active 45h

# 3 指定输出格式 命令：kubectl get ns ns名称 -o 格式参数
# kubernetes支持的格式有很多，比较常见的是wide、json、yaml
[root@master ~]# kubectl get ns default -o yaml
apiVersion: v1
kind: Namespace
metadata:
creationTimestamp: "2021-05-08T04:44:16Z"
name: default
resourceVersion: "151"
selfLink: /api/v1/namespaces/default
uid: 7405f73a-e486-43d4-9db6-145f1409f090
spec:
finalizers:
- kubernetes
status:
phase: Active

# 4 查看ns详情 命令：kubectl describe ns ns名称
[root@master ~]# kubectl describe ns default
Name: default
Labels: <none>
Annotations: <none>
Status: Active # Active 命名空间正在使用中 Terminating 正在删除命名空间

# ResourceQuota 针对namespace做的资源限制
# LimitRange针对namespace中的每个组件做的资源限制
No resource quota.
No LimitRange resource.


# 创建namespace
[root@master ~]# kubectl create ns dev
namespace/dev created

# 删除namespace
[root@master ~]# kubectl delete ns dev
namespace "dev" deleted

```

配置方式 首先准备一个yaml文件：ns-dev.yaml

```yaml 
apiVersion: v1
kind: Namespace
metadata:
  name: dev
```

然后就可以执行对应的创建和删除命令了： 

```
创建：kubectl create -f ns-dev.yaml 

删除：kubectl delete -f ns-dev.yaml

```



# 容器运行时接口（CRI）

CRI 是一个插件接口，它使 kubelet 能够使用各种容器运行时，无需重新编译集群组件。

你需要在集群中的每个节点上都有一个可以正常工作的[容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes)， 这样 [kubelet](https://kubernetes.io/docs/reference/generated/kubelet) 能启动 [Pod](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/) 及其容器。



容器运行时接口（CRI）是 kubelet 和容器运行时之间通信的主要协议。



Kubernetes 容器运行时接口（Container Runtime Interface；CRI）定义了主要 [gRPC](https://grpc.io/) 协议， 用于[集群组件](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#node-components) [kubelet](https://kubernetes.io/docs/reference/generated/kubelet) 和 [容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes)之间的通信。

![img](https://static001.infoq.cn/resource/image/7d/68/7dcb295774caa7c68f9e7b7e3322a368.png)



# Pod 

Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod 中。 

Pod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。

docker调度的是容器，在k8s集群中，最小的调度单元是Pod（豆荚）

![](http://cdn.gtrinee.top/pod-demo.png)

## 为什么Kubernetes要引入pod的概念，而不直接操作 Docker容器

- 第一个原因：借助CRI这个抽象层，使得Kubernetes不依赖于底层某一种具体的容器运行时实现技 术，而是直接操作pod，pod内部再管理多个业务上紧密相关的用户业务容器，这种架构便于 Kubernetes做扩展。 
- 第二个原因，我们假设Kubernetes没有pod的概念，而是直接管理容器，那么一组容器作为一个单 元，假设其中一个容器死亡了，此时这个单元的状态应该如何定义呢？应该理解成整体死亡，还是 个别死亡？
- 第三个原因：pod里所有的业务容器共享pause容器的IP地址，以及pause容器mount的Volume， 通过这种设计，业务容器之间可以直接通信，文件也能够直接彼此共享。

###### 为什么引入Pod

- 与容器引擎解耦

  Docker、Rkt。平台设计与引擎的具体的实现解耦

- 多容器共享网络|存储|进程 空间, 支持的业务场景更加灵活

**Pod的常用资源清单**

```bash
apiVersion: v1                             # 必选，版本号v1
kind: Pod                                  # 必选，资源类型Pod
metadata:       　                         # 必选，元数据
  name: string                             # 必选，Pod名称
  namespace: string                        # Pod所属的命名空间, 默认为"default"
  labels:       　　                       # 自定义标签列表
    - name: string      　          
spec:                                      # 必选，Pod中容器的详细定义
  containers:                              # 必选，Pod中容器列表
  - name: string                           # 必选，容器名称
    image: string                          # 必选，容器的镜像名称
    imagePullPolicy: [Always|Never|IfNotPresent]          # 获取镜像的策略 
    command: [string]                      # 容器的启动命令列表，默认是打包时使用的启动命令
    args: [string]                         # 容器的启动命令参数列表
    workingDir: string                     # 容器的工作目录
    volumeMounts:                          # 挂载到容器内部的存储卷配置
    - name: string                         # 引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名
      mountPath: string                    # 存储卷在容器内mount的绝对路径，应少于512字符
      readOnly: boolean                    # 是否为只读模式
    ports:                                 # 需要暴露的端口号列表
    - name: string                         # 端口的名称
      containerPort: int                   # 容器需要监听的端口号
      hostPort: int                        # 容器所在主机需要监听的端口号，默认与Container相同
      protocol: string                     # 端口协议，支持TCP和UDP，默认TCP
    env:                                   # 容器运行前需设置的环境变量列表
    - name: string                         # 环境变量名称
      value: string                        # 环境变量的值
    resources:                             # 资源限制和请求的设置
      limits:                              # 资源限制的设置
        cpu: string                        # Cpu的限制，单位为core数，将用于docker run --cpu-shares参数
        memory: string                     # 内存限制，单位可以为Mib/Gib，将用于docker run --memory参数
      requests:                            # 资源请求的设置
        cpu: string                        # Cpu请求，容器启动的初始可用数量
        memory: string                     # 内存请求,容器启动的初始可用数量
    lifecycle:                             # 生命周期钩子
		postStart:                         # 容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启
		preStop:                           # 容器终止前执行此钩子,无论结果如何,容器都会终止
    livenessProbe:                         # 对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器
      exec:       　                       # 对Pod容器内检查方式设置为exec方式
        command: [string]                  # exec方式需要制定的命令或脚本 执行的第一个命令
      httpGet:                             # 对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port
        path: string
        port: number
        host: string
        scheme: string
        HttpHeaders:
        - name: string
          value: string
      tcpSocket:                           # 对Pod内的容器健康检查方式设置为tcpSocket方式
         port: number
       initialDelaySeconds: 0              # 容器启动完成后首次探测的时间，单位为秒
       timeoutSeconds: 0    　　           # 对容器健康检查探测等待响应的超时时间，单位秒，默认1秒
       periodSeconds: 0     　　           # 对容器健康检查的定期探测时间设置，单位秒，默认10秒一次
       successThreshold: 0
       failureThreshold: 0
       securityContext:
         privileged: false
  restartPolicy: [Always|Never|OnFailure]          # Pod的重启策略
  nodeName: <string>                       # 设置NodeName表示将该Pod调度到指定到名称的node节点上
  nodeSelector: obeject                    # 设置NodeSelector表示将该Pod调度到包含这个label的node上
  imagePullSecrets:                        # Pull镜像时使用的secret名称，以key：secretkey格式指定
  - name: string
  hostNetwork: false                       # 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络
  volumes:                                 # 在该pod上定义共享存储卷列表
  - name: string                           # 共享存储卷名称 (volumes类型有很多种)
    emptyDir: {}                           # 类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值
    hostPath: string                       # 类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录
      path: string      　　               # Pod所在宿主机的目录，将被用于同期中mount的目录
    secret:       　　　                    # 类型为secret的存储卷，挂载集群与定义的secret对象到容器内部
      scretname: string  
      items:     
      - key: string
        path: string
    configMap:                             # 类型为configMap的存储卷，挂载预定义的configMap对象到容器内部
      name: string
      items:
      - key: string
        path: string

```



###### 使用yaml格式定义Pod

*myblog/one-pod/pod.yaml*

```yaml
apiVersion: v1
kind: Pod
metadata:
name: nginx
namespace: dev
spec:
containers:
- image: nginx:latest
name: pod
ports:
- name: nginx-port
containerPort: 80
protocol: TCP
```





###### 创建和访问Pod

```powershell
## 创建namespace, namespace是逻辑上的资源池
$ kubectl create namespace demo

## 使用指定文件创建Pod
$ kubectl create -f demo-pod.yaml

## 查看pod，可以简写po
## 所有的操作都需要指定namespace，如果是在default命名空间下，则可以省略
$ kubectl -n demo get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE    IP             NODE
myblog   2/2     Running   0          3m     10.244.1.146   k8s-slave1

## 使用Pod Ip访问服务,3306和8002
$ curl 10.244.1.146:8002/blog/index/

## 进入容器,执行初始化, 不必到对应的主机执行docker exec
$ kubectl -n demo exec -ti myblog -c myblog bash
/ # env
/ # python3 manage.py migrate
$ kubectl -n demo exec -ti myblog -c mysql bash
/ # mysql -p123456

## 再次访问服务,3306和8002
$ curl 10.244.1.146:8002/blog/index/

```

###### 使用yaml创建出现的问题

1.首先需要将原有的镜像打tag push到私有仓库里

部署镜像仓库

https://docs.docker.com/registry/ 

```powershell
## 使用docker镜像启动镜像仓库服务
$ docker run -d -p 5000:5000 --restart always -v /opt/registry-data/registry:/var/lib/registry --name registry registry:2

## 默认仓库不带认证，若需要认证，参考https://docs.docker.com/registry/deploying/#restricting-access
```

4. 推送本地镜像到镜像仓库中

   ```powershell
   $ docker tag nginx:alpine localhost:5000/nginx:alpine
   $ docker push localhost:5000/nginx:alpine
   ## 我的镜像仓库给外部访问，不能通过localhost，尝试使用内网地址172.21.16.3:5000/nginx:alpine
   $ docker tag nginx:alpine 172.21.16.3:5000/nginx:alpine
   $ docker push 172.21.16.3:5000/nginx:alpine
   The push refers to repository [172.21.16.3:5000/nginx]
   Get https://172.21.16.3:5000/v2/: http: server gave HTTP response to HTTPS client
   ## docker默认不允许向http的仓库地址推送，如何做成https的，参考：https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry
   ## 我们没有可信证书机构颁发的证书和域名，自签名证书需要在每个节点中拷贝证书文件，比较麻烦，因此我们通过配置daemon的方式，来跳过证书的验证：
   $ cat /etc/docker/daemon.json
   {
     "registry-mirrors": [
       "https://8xpk5wnt.mirror.aliyuncs.com"
     ],
     "insecure-registries": [
        "172.21.16.3:5000"
     ]
   }
   $ systemctl restart docker
   $ docker push 172.21.16.3:5000/nginx:alpine
   $ docker images	# IMAGE ID相同，等于起别名或者加快捷方式
   REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
   172.21.16.3:5000/nginx   alpine              377c0837328f        4 weeks ago         
   nginx                    alpine              377c0837328f        4 weeks ago         
   localhost:5000/nginx     alpine              377c0837328f        4 weeks ago         
   registry                 2                   708bc6af7e5e        2 months ago       
   ```

5. ![image-20230311222523150](http://cdn.gtrinee.top/image-20230311222523150.png)

6. 在node节点同样在配置跳过证书验证

![image-20230311222540979](http://cdn.gtrinee.top/image-20230311222540979.png)

![image-20230311231658001](http://cdn.gtrinee.top/image-20230311231658001.png)



###### Infra容器

登录`k8s-slave1`节点

```powershell
$ docker ps -a |grep myblog  ## 发现有三个容器
## 其中包含mysql和myblog程序以及Infra容器
## 为了实现Pod内部的容器可以通过localhost通信，每个Pod都会启动Infra容器，然后Pod内部的其他容器的网络空间会共享该Infra容器的网络空间(Docker网络的container模式)，Infra容器只需要hang住网络空间，不需要额外的功能，因此资源消耗极低。

## 登录master节点，查看pod内部的容器ip均相同，为pod ip
$ kubectl -n demo exec -ti myblog -c myblog bash
/ # ifconfig
$ kubectl -n demo exec -ti myblog -c mysql bash
/ # ifconfig
```

pod容器命名: ```k8s_<container_name>_<pod_name>_<namespace>_<random_string>```

###### 查看pod详细信息

```powershell
## 查看pod调度节点及pod_ip
$ kubectl -n demo get pods -o wide
## 查看完整的yaml
$ kubectl -n demo get po myblog -o yaml
## 查看pod的明细信息及事件
$ kubectl -n demo describe pod myblog
```



## 多容器Pod的优缺点

[Multi container pod多容器Pod-CSDN博客](https://blog.csdn.net/mygugu/article/details/123629808)

## 为什么使用多容器pod?

比如有一个application需要多个容器运行在相同Host上时，最好的选择就是多容器pod，尽管这会违反“One process per container” 规则，也不利于排查问题，但是利大于弊，例如团队内部可以重用更精细的容器。

[【Kubernetes】Pod 之 多容器 - 掘金 (juejin.cn)](https://juejin.cn/post/7083853966664007717)

**为什么在一个`pod`上运行多个容器，而不是在一个容器上运行多个进程呢？**

1. 容器的设计原则就是一个容器运行一个进程（由父进程产生子进程的除外）
2. 容器中有多个进程，根据日志进行系统排错变困难，难以管理不同进程的生命周期
3. 一个应用程序使用多个容器可以实现进一步的解耦

**多容器Pod的特征**：

运行在一个`pod`节点上的多个容器，它们共享的资源有：

- 相同的`Linux`命名空间（相同`IP`地址和端口空间，容器间互相访问）
- 相同 `IPC`名称空间
- 可以使用相同的共享卷

**使用场景**：应用由一个主进程和一个/多个辅助进程组成。

### `Pod`容器间的通信

**通信主要3种方式：**

- `Pod` 共享 `Volume`
- 进程间通信（`IPC`）
- 容器间网络通信



### （1）`Pod` 共享 `Volume`

常见用法： 一个容器写日志或其它数据文件到共享目录中，其它容器从这个共享目录中读取数据。

`Pod` 中运行的多个容器可以共享 `Pod` 级别的存储卷 `Volume`，可以使用 `emptyDir` 类型的存储卷作为 `Pod` 内部的空目录，用于 `Pod` 中运行的多个容器间的数据共享。

举个例子：

一个`pod`包含两个容器：

- `tomcat`容器，用于向`app-logs` `volume`中写入日志文件
- `busybox`容器用于从`app-logs` `volume`中读取日志文件并打印

1. 创建`tomcat-busybox.yaml`

> `tomcat`容器在启动后会向`/usr/local/tomcat/logs`目录下写日志文件，而`busybox`容器就可以读取其中的日志文件

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tomcat-busybox-pod
spec:
  containers:
    - name: tomcat
    image: tomcat
      ports:
        - containerPort: 8080
      volumeMounts:
        - name: app-logs
          mountPath: /usr/local/tomcat/logs # 将共享卷 app-logs 挂载到 tomcat 容器内的 /usr/local/tomcat/logs 目录下
    - name: busybox
      image: busybox
      command: ["sh", "-c", "tail -f /logs/catalina*.log"]
      volumeMounts:
        - name: app-logs
          mountPath: /logs # 将共享卷 app-logs 挂载到 busybox 容器内的 /logs 目录下
  volumes:
    - name: app-logs
      emptyDir: {}
复制代码
```

1. 执行创建

```shell
$ kubectl create -f tomcat-busybox.yaml
pod/tomcat-busybox-pod created

# 这里的 2/2 表示两个容器都启动成功了
$ kubectl get pods
NAME                 READY   STATUS    RESTARTS   AGE
tomcat-busybox-pod   2/2     Running   0          50s
复制代码
```

1. 查看创建过程

```shell
# 查看详细的创建过程（包括 Volumes 和 Events），可以看到两个容器依次被创建了起来
$ kubectl describe pod tomcat-busybox-pod
复制代码
```

1. 查看`busybox`日志

> `busybox`容器的启动命令`tail -f /logs/catalina*.log` 通过`kubelet logs`命令查看`busybox`容器的输出内容 `busybox`容器打印的是日志文件 /usr/local/tomact/logs/catalina..log 中的内容

```shell
$ kubectl logs tomcat-busybox-pod -c busybox
......
26-Sep-2019 09:56:26.653 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory [/usr/local/tomcat/webapps/docs]
26-Sep-2019 09:56:26.687 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/tomcat/webapps/docs] has finished in [34] ms
26-Sep-2019 09:56:26.691 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
26-Sep-2019 09:56:26.702 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
26-Sep-2019 09:56:26.705 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 822 ms
复制代码
```

1. 查看`tomcat`容器日志信息

```shell
$ kubectl exec -it tomcat-busybox-pod -c tomcat -- ls /usr/local/tomcat/logs
catalina.2019-09-26.log      localhost_access_log.2019-09-26.txt
host-manager.2019-09-26.log  manager.2019-09-26.log
localhost.2019-09-26.log

$ kubectl exec -it tomcat-busybox-pod -c tomcat -- tail /usr/local/tomcat/logs/catalina.2019-09-26.log
......
26-Sep-2019 09:56:26.653 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory [/usr/local/tomcat/webapps/docs]
26-Sep-2019 09:56:26.687 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/tomcat/webapps/docs] has finished in [34] ms
26-Sep-2019 09:56:26.691 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
26-Sep-2019 09:56:26.702 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
26-Sep-2019 09:56:26.705 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 822 ms
复制代码
```

1. 删除 `pod`：`kubectl delete pod xxx`



### （2）进程间通信（`IPC`）

一个 `Pod` 中的多个容器之间共享相同的 `IPC` 命名空间，所以它们互相之间可以使用标准进程间通信，比如：`SystemV` 信号系统或 `POSIX` 共享内存。

**举个例子：**

**在一个`pod`中运行两个容器：**

- 生产者`producer`
- 消费者`consumer`

> **这两个容器都使用相同的镜像`ipc`**：
>
> 1. `producer`容器创建了一个标准的`Linux`消费队列，写一些随机的消息，最后写一个特殊的退出消息
> 2. `consumer`容器打开相同的消费队列来读取消息，直到接收到退出消息。

1. 创建`yaml`配置文件

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ipc-test
spec:
  containers:
    - name: producer
      image: registry-vpc.cn-hangzhou.aliyuncs.com/chenshi-kubernetes/ipc:latest
      command: ["./ipc", "-producer"]
    - name: consumer
      image: registry-vpc.cn-hangzhou.aliyuncs.com/chenshi-kubernetes/ipc:latest
      command: ["./ipc", "-consumer"]
  restartPolicy: Never
复制代码
```

1. 执行创建：`kubectl create -f ipc-pod.yaml`
2. 查看每个容器的日志

```shell
$ kubectl logs ipc-test -c producer
......
Produced: 9a
Produced: 21
Produced: c8
Produced: a
Produced: 76

$ kubectl logs ipc-test -c consumer
......
Consumed: 9a
Consumed: 21
Consumed: c8
Consumed: a
Consumed: 76
Consumed: done
复制代码
```



### （3）容器间网络通信

> `pod`中的多个容器都可以通过`localhost`进行互相访问，因为它们都使用相同的网络名称空间。

**举个例子：**

**在`pod`中创建两个容器：**

- `nginx`容器作为反向代理
- `webapp`容器运行一个简单的服务

> 配置`nginx`文件通过 http 80端口访问请求会被转发到本地的 5000 端口上

1. 在`nginx.conf`配置文件写入

```conf
user nginx;
worker_processes 1;

error_log /var/log/nginx/error.log warn;
pid       /var/run/nginx.pid;

events {
worker_connections 1024;
}

http {
include      /etc/nginx/mime.types;
default_type application/octet-stream;

sendfile          on;
keepalive_timeout 65;

upstream webapp {
server 127.0.0.1:5000;
}

server {
listen 80;

location / {
proxy_pass     http://webapp;
proxy_redirect off;
}
}
}
复制代码
```

1. 使用`nginx.conf`配置文件创建一个 `nginx-conf`的`ConfigMap`

```shell
$ kubectl create configmap nginx-conf --from-file=/home/shiyanlou/nginx.conf
configmap/nginx-conf created
复制代码
```

1. 创建`pod`文件

> 开放 `nginx`容器的 80 端口，`webapp`容器的5000端口没有开放的，不能被`pod`外部访问

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-webapp-pod
  labels:
    app: http-service
spec:
  containers:
    - name: webapp
      image: registry-vpc.cn-hangzhou.aliyuncs.com/chenshi-kubernetes/webapp:latest
    - name: nginx
      image: nginx:alpine
      ports:
        - containerPort: 80
      volumeMounts:
        - name: nginx-proxy-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
  volumes:
    - name: nginx-proxy-config
      configMap:
        name: nginx-conf
复制代码
```

1. 执行创建`pod`资源对象

```shell
$ kubectl create -f nginx-webapp.yaml
pod/nginx-webapp-pod created
复制代码
```

1. 使用`NodePort`服务暴露`nginx-webapp-pod Pod`

```shell
$ kubectl expose pod nginx-webapp-pod --type=NodePort --port=80
service/nginx-webapp-pod exposed
复制代码
```

1. 查看 `nginx-webapp-pod`所在的`Node`节点，并且查看`nginx-webapp-pod`服务开放的`NodePort`

```shell
$ kubectl get pod nginx-webapp-pod -o wide
NAME               READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES
nginx-webapp-pod   2/2     Running   0          7m7s   10.244.2.4   kube-node-1   <none>           <none>

$ kubectl describe service nginx-webapp-pod
...
NodePort:                 <unset>  30889/TCP
...
复制代码
```

1. 通过`curl`访问 `Node IP + NodePort`地址

```shell
$ curl 10.192.0.3:30889
Hello world!
```







# Pod数据持久化

[(19条消息) k8s 数据存储及配置存储_k8s存储配置_小毕超的博客-CSDN博客](https://blog.csdn.net/qq_43692950/article/details/119857302)

在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器 中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数 据，kubernetes引入了Volume的概念。 

Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂 载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据 的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时， Volume中的数据也不会丢失。 

kubernetes的Volume支持多种类型，比较常见的有下面几个： 

- 简单存储：EmptyDir、HostPath、NFS 
- 高级存储：PV、PVC 
- 配置存储：ConfigMap、Secret

### EmptyDir

EmptyDir是最基础的Volume类型，**一个EmptyDir就是Host上的一个空目录。**

EmptyDir用途如下： 

- 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留 
- 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）

在yaml中这样写

```yaml
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
    ports:
    - containerPort: 80
    volumeMounts: # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx
    - name: logs-volume
      mountPath: /var/log/nginx
  - name: busybox
  image: busybox:1.30
  command: ["/bin/sh","-c","tail -f /logs/access.log"] # 初始命令，动态读取指
    定文件中内容
  volumeMounts: # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs
  - name: logs-volume
    mountPath: /logs
  volumes: # 声明volume， name为logs-volume，类型为emptyDir
  - name: logs-volume
  emptyDir: {}

```

### HostPath

上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想**简单的将数据持久化 到主机中**，可以选择HostPath。 HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销 毁了，但是数据依据可以存在于Node主机上。

![image-20230309145728949](http://cdn.gtrinee.top/image-20230309145728949.png)

创建一个volume-hostpath.yaml：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-hostpath
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
    ports:
    - containerPort: 80
    volumeMounts:
    - name: logs-volume
      mountPath: /var/log/nginx
  - name: busybox
    image: busybox:1.30
    command: ["/bin/sh","-c","tail -f /logs/access.log"]
    volumeMounts:
	- name: logs-volume
	  mountPath: /logs
  volumes:
  - name: logs-volume
	hostPath:
	  path: /root/logs
	  type: DirectoryOrCreate # 目录存在就使用，不存在就先创建后使用
```

```
关于type的值的一点说明：
    DirectoryOrCreate 目录存在就使用，不存在就先创建后使用
    Directory 目录必须存在
    FileOrCreate 文件存在就使用，不存在就先创建后使用
    File 文件必须存在
    Socket unix套接字必须存在
    CharDevice 字符设备必须存在
    BlockDevice 块设备必须存在
```

```bash
# 创建Pod
[root@master ~]# kubectl create -f volume-hostpath.yaml
pod/volume-hostpath created

# 查看Pod
[root@master ~]# kubectl get pods volume-hostpath -n dev -o wide
NAME READY STATUS RESTARTS AGE IP NODE
NOMINATED NODE READINESS GATES
volume-hostpath 2/2 Running 0 17s 10.244.1.7 node1
<none> <none>

#访问nginx
[root@master ~]# curl 10.244.1.7

# 接下来就可以去host的/root/logs目录下查看存储的文件了
### 注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）
[root@node1 ~]# ls /root/logs/
access.log error.log
[root@node1 ~]# cat logs/access.log
10.244.0.0 - - [21/Oct/2021:12:35:28 +0000] "GET / HTTP/1.1" 200 612 "-"
"curl/7.29.0" "-"

# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的
```

### NFS

 HostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会 出现问题了，此时需要准备单独的网络存储系统，比较常用的用**NFS、CIFS。** 

NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上， 这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。

![image-20230309150840486](http://cdn.gtrinee.top/image-20230309150840486.png)



1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器

```bash
# 在nfs上安装nfs服务
[root@master ~]# yum install nfs-utils -y

# 准备一个共享目录
[root@master ~]# mkdir /root/data/nfs -pv
mkdir: 已创建目录 "/root/data"
mkdir: 已创建目录 "/root/data/nfs"

# 将共享目录以读写权限暴露给192.168.175.0/24网段中的所有主机
[root@master ~]# vim /etc/exports
/root/data/nfs 192.168.175.0/24(rw,no_root_squash)
# 启动nfs服务
[root@master ~]# systemctl restart nfs

```

接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备

```bash
# 在node上安装nfs服务，注意不需要启动
[root@node1 ~]# yum install nfs-utils -y

```

3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: volume-nfs
    namespace: dev
spec:
    containers:
    - name: nginx
      image: nginx:1.17.1
      ports:
      - containerPort: 80
      volumeMounts:
      - name: logs-volume
        mountPath: /var/log/nginx
    - name: busybox
      image: busybox:1.30
      command: ["/bin/sh","-c","tail -f /logs/access.log"]
      volumeMounts:
      - name: logs-volume
        mountPath: /logs
    volumes:
    - name: logs-volume
    nfs:
      server: 192.168.175.100 #nfs服务器地址
      path: /root/data/nfs #共享文件路径

```

## 高级存储

前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。

**PV（Persistent Volume）** 是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。

**PVC（Persistent Volume Claim）** 是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。

![在这里插入图片描述](https://img-blog.csdnimg.cn/57441b070f4944d5a0ac1fd50fef5e75.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNjkyOTUw,size_16,color_FFFFFF,t_70#pic_center)

使用了PV和PVC之后，工作可以得到进一步的细分：

- 存储：存储工程师维护
- PV： kubernetes管理员维护
- PVC：kubernetes用户维护

### 持久化存储之Storageclass

[kubernetes中持久化存储之Storageclass - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/104555373)

[存储类 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/)



# 云原生存储 OpenEBS

[全网最全的云原生存储 OpenEBS 使用指南 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/519172233)

OpenEBS 将可用于 Kubernetes 工作节点的任何存储转换为本地或分布式 Kubernetes 持久化卷（Persistent Volume）。

OpenEBS 帮助应用程序和平台团队轻松地部署 Kubernetes 有状态工作负载，这些工作负载需要快速、高度持久、可靠和可扩展的容器附加存储（[Container Attached Storage](https://openebs.io/docs/concepts/cas)）。

OpenEBS 也是基于 NVMe 的存储部署的首选。

OpenEBS 最初由 [MayaData](https://mayadata.io/) 构建，后来被捐赠给云原生计算基金会，现在是 CNCF 沙盒项目。

## 安装

[安装OpenEBS (timd.cn)](http://timd.cn/k8s/openebs-installation/)

## 配置存储

### ConfigMap

环境变量中敏感信息带来的安全隐患

为什么要统一管理环境变量

- 环境变量中有很多敏感的信息，比如账号密码，直接暴漏在yaml文件中存在安全性问题
- 团队内部一般存在多个项目，这些项目直接存在配置相同环境变量的情况，因此可以统一维护管理
- 对于开发、测试、生产环境，由于配置均不同，每套环境部署的时候都要修改yaml，带来额外的开销

k8s提供两类资源，configMap和Secret，可以用来实现业务配置的统一管理， 允许将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性 。

![](http://cdn.gtrinee.top/configmap.png)

- configMap，通常用来管理应用的配置文件或者环境变量，`myblog/two-pod/configmap.yaml`

  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: myblog
    namespace: demo
  data:
    MYSQL_HOST: "172.21.32.6"
    MYSQL_PORT: "3306"
    
  # 创建configmap
  [root@master ~]# kubectl create -f configmap.yaml
  configmap/configmap created
  # 查看configmap详情
  [root@master ~]# kubectl describe cm configmap -n dev
  ```

- Secret，管理敏感类的信息，默认会base64编码存储，有三种类型

  - Service Account ：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；创建ServiceAccount后，Pod中指定serviceAccount后，自动创建该ServiceAccount对应的secret；

  - Opaque ： base64编码格式的Secret，用来存储密码、密钥等；

  - ```bash
    [root@master ~]# echo -n 'admin' | base64 #准备username
    YWRtaW4=
    [root@master ~]# echo -n '123456' | base64 #准备password
    MTIzNDU2
    ```

  - kubernetes.io/dockerconfigjson ：用来存储私有docker registry的认证信息。

`myblog/two-pod/secret.yaml`

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: myblog
  namespace: demo
type: Opaque
data:
  MYSQL_USER: cm9vdA==		#注意加-n参数， echo -n root|base64
  MYSQL_PASSWD: MTIzNDU2
```

创建并查看：

```powershell
$ kubectl create -f secret.yaml
$ kubectl -n demo get secret
```

如果不习惯这种方式，可以通过如下方式：

```powershell
$ cat secret.txt
MYSQL_USER=root
MYSQL_PASSWD=123456
$ kubectl -n demo create secret generic myblog --from-env-file=secret.txt 
```

修改后的mysql的yaml，资源路径：`myblog/two-pod/mysql-with-config.yaml`

```yaml
...
spec:
  containers:
  - name: mysql
    image: 172.21.32.6:5000/mysql:5.7-utf8
    env:
    - name: MYSQL_USER
      valueFrom:
        secretKeyRef:
          name: myblog
          key: MYSQL_USER
    - name: MYSQL_PASSWD
      valueFrom:
        secretKeyRef:
          name: myblog
          key: MYSQL_PASSWD
    - name: MYSQL_DATABASE
      value: "myblog"
...
```

# pod对象的生命周期

- 我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程： 
- pod创建过程 
- 运行初始化容器（init container）过程 
- 运行主容器（main container） 
  - 容器启动后钩子（post start）、容器终止前钩子（pre stop） 
  - 容器的存活性探测（liveness probe）、就绪性探测（readiness probe） 
- pod终止过程

![image-20230309215847740](http://cdn.gtrinee.top/image-20230309215847740.png)

## pod的创建过程

1. 用户通过kubectl或其他api客户端提交pod spec给api server 
2. api server尝试着将pod对象的相关信息存入etcd中，待写入操作执行完成，api server即会返回确 认信息至客户端 
3. api server开始反应etcd中的状态变化 
4. 所有的kubernetes组件均使用watch机制来跟踪检查api server上的相关的变动 
5. kube-scheduler（调度器）通过其watcher觉察到api server创建了新的pod对象但尚未绑定至任 何工作节点 
6. kube-scheduler为pod对象挑选一个工作节点并将结果信息更新至api server 
7. 调度结果信息由api server更新至etcd存储系统中，而却api server也开始反映此pod对象的调度结 果 
8. pod被调度到的目标工作节点上的kubelet尝试在当前节点上调用docker启动容器，并将容器的结 果状态回送至api server 
9. api server将pod状态信息存入etcd中
10.  在etcd确认写入操作成功完成之后，api server将确认信息发送至相关的kubelet事件将通过它被接 收

![image-20230309220220529](http://cdn.gtrinee.top/image-20230309220220529.png)

## 服务健康检查

检测容器服务是否健康的手段，若不健康，会根据设置的重启策略（restartPolicy）进行操作，两种检测机制可以分别单独设置，若不设置，默认认为Pod是健康的。

两种机制：

> livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。

- LivenessProbe探针:：**存活性探针**，用于检测应用实例当前**是否处于正常运行状态**，如果不是，k8s会 重启容器
  用于判断容器**是否存活**，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。 
- ReadinessProbe探针:**就绪性探针**，用于检测应用实例**当前是否可以接收请求**，如果不能，k8s不会 转发流l量
  用于判断容器是否正常提供服务，即容器的Ready是否为True，是否可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。（剔除此pod不参与接收请求不会将流量转发给此Pod）。

#### 钩子函数

钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。 

kubernetes在主容器的启动之后和停止之前提供了两个钩子函数： 

- post start：容器创建之后执行，如果失败了会重启容器 

- pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操 作 

钩子处理器支持使用下面三种方式定义动作：

三种类型：

- exec：通过执行命令来检查服务是否正常，回值为0则表示容器健康

- ```yaml
  …
  lifecycle:
    postStart:
      exec:
        command:
  	  - cat
  	  - /tmp/healthy
  ```

- httpGet方式：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康

- ```yaml
  lifecycle:
    postStart:
  	httpGet:
  	  path: / #URI地址
        port: 80 #端口号
        host: 192.168.5.3 #主机地址
        scheme: HTTP #支持的协议，http或者https
  ```

- tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康

- ```yaml
  lifecycle:
    postStart:
      tcpSocket:
        port: 8080
  ```

  

示例：

完整文件路径 `myblog/one-pod/pod-with-healthcheck.yaml`



# Label

Label是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和 选择。 

Label的特点：

- 一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等 
- 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去 
- Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除

命令方式

```bash
# 为pod资源打标签
[root@master ~]# kubectl label pod nginx version=1.0 -n dev
pod/nginx-pod labeled
# 为pod资源更新标签
[root@master ~]# kubectl label pod nginx version=2.0 -n dev --overwrite
pod/nginx-pod labeled
# 查看标签
[root@master ~]# kubectl get pod nginx-pod -n dev --show-labels
NAME READY STATUS RESTARTS AGE LABELS
nginx-pod 1/1 Running 0 10m version=2.0
# 筛选标签
[root@master ~]# kubectl get pod -n dev -l version=2.0 --show-labels
NAME READY STATUS RESTARTS AGE LABELS
nginx-pod 1/1 Running 0 17m version=2.0
[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels
No resources found in dev namespace.
#删除标签
[root@master ~]# kubectl label pod nginx-pod version- -n dev
pod/nginx-pod labeled

```

配置方式

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: nginx
    namespace: dev
    labels:
        version: "3.0"
        env: "test"
spec:
    containers:
    - image: nginx:latest
        name: pod
        ports:
            - name: nginx-port
            containerPort: 80
            protocol: TCP

```

###### 小结

1. 实现k8s平台与特定的容器运行时解耦，提供更加灵活的业务部署方式，引入了Pod概念
2. k8s使用yaml格式定义资源文件，yaml中Map与List的语法，与json做类比
3. 通过kubectl create | get | exec | logs | delete 等操作k8s资源，必须指定namespace
4. 每启动一个Pod，为了实现网络空间共享，会先创建Infra容器，并把其他容器网络加入该容器
5. 通过livenessProbe和readinessProbe实现Pod的存活性和就绪健康检查
6. 通过requests和limit分别限定容器初始资源申请与最高上限资源申请
7. 通过Pod IP访问具体的Pod服务，实现是



# Deployment

只使用Pod, 将会面临如下需求:

1. 业务应用启动多个副本
2. Pod重建后IP会变化，外部如何访问Pod服务
3. 运行业务Pod的某个节点挂了，可以自动帮我把Pod转移到集群中的可用节点启动起来
4. 我的业务应用功能是收集节点监控数据,需要把Pod运行在k8集群的各个节点上



###### Workload (工作负载)

控制器又称工作负载是用于实现管理pod的中间层，确保pod资源符合预期的状态，pod的资源出现故障时，会尝试 进行重启，当根据重启策略无效，则会重新新建pod的资源。 

![](http://cdn.gtrinee.top/workload.png)



- ReplicaSet: 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能
- Deployment：工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置
- DaemonSet：用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如ELK服务
- Job：只要完成就立即退出，不需要重启或重建
- Cronjob：周期性任务控制，不需要持续后台运行
- StatefulSet：管理有状态应用

在kubernetes中Pod控制器的种 类有很多，本章节只介绍一种：Deployment。

Deployment主要功能有下面几个： 

- 支持ReplicaSet的所有功能 
- 支持发布的停止、继续 
- 支持滚动升级和回滚版本

![image-20230309225030815](http://cdn.gtrinee.top/image-20230309225030815.png)



配置操作 创建一个deploy-nginx.yaml，内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: dev
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
    labels:
      run: nginx
    spec:
      containers:
      - image: nginx:latest
        name: nginx
    	ports:
	  - containerPort: 80
		protocol: TCP
```

###### 创建Deployment

```powershell
# 创建deployment
[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true
deployment.apps/pc-deployment created
# 查看deployment
# UP-TO-DATE 最新版本的pod的数量
# AVAILABLE 当前可用的pod的数量
[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev
NAME READY UP-TO-DATE AVAILABLE AGE
pc-deployment 3/3 3 3 15s
# 查看rs
# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串
[root@k8s-master01 ~]# kubectl get rs -n dev
NAME DESIRED CURRENT READY AGE
pc-deployment-6696798b78 3 3 3 23s
# 查看pod
[root@k8s-master01 ~]# kubectl get pods -n dev
NAME READY STATUS RESTARTS AGE
pc-deployment-6696798b78-d2c8n 1/1 Running 0 107s
pc-deployment-6696798b78-smpvp 1/1 Running 0 107s
pc-deployment-6696798b78-wvjd8 1/1 Running 0 107s

```

###### 查看Deployment

```powershell
# kubectl api-resources
$ kubectl -n demo get deploy
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
myblog   1/1     1            1           2m22s
mysql    1/1     1            1           2d11h

  * `NAME` 列出了集群中 Deployments 的名称。
  * `READY`显示当前正在运行的副本数/期望的副本数。
  * `UP-TO-DATE`显示已更新以实现期望状态的副本数。
  * `AVAILABLE`显示应用程序可供用户使用的副本数。
  * `AGE` 显示应用程序运行的时间量。

# 查看pod
$ kubectl -n demo get po
NAME                      READY   STATUS    RESTARTS   AGE
myblog-7c96c9f76b-qbbg7   1/1     Running   0          109s
mysql-85f4f65f99-w6jkj    1/1     Running   0          2m28s
```

**扩缩容**

```bash
# 变更副本数量为5个
[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5 -n
dev
deployment.apps/pc-deployment scaled

# 查看deployment
[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev
NAME READY UP-TO-DATE AVAILABLE AGE
pc-deployment 5/5 5 5 2m

# 查看pod
[root@k8s-master01 ~]# kubectl get pods -n dev
NAME READY STATUS RESTARTS AGE
pc-deployment-6696798b78-d2c8n 1/1 Running 0 4m19s
pc-deployment-6696798b78-jxmdq 1/1 Running 0 94s
pc-deployment-6696798b78-mktqv 1/1 Running 0 93s
pc-deployment-6696798b78-smpvp 1/1 Running 0 4m19s
pc-deployment-6696798b78-wvjd8 1/1 Running 0 4m19s

# 编辑deployment的副本数量，修改spec:replicas: 4即可
[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev
deployment.apps/pc-deployment edited

# 查看pod
[root@k8s-master01 ~]# kubectl get pods -n dev
NAME READY STATUS RESTARTS AGE
pc-deployment-6696798b78-d2c8n 1/1 Running 0 5m23s
pc-deployment-6696798b78-jxmdq 1/1 Running 0 2m38s
pc-deployment-6696798b78-smpvp 1/1 Running 0 5m23s
pc-deployment-6696798b78-wvjd8 1/1 Running 0 5m23s
```

**镜像更新** 

deployment支持两种更新策略: **重建更新 和 滚动更新** ,可以通过 strategy 指定策略类型,支持两个属性:

```
strategy：指定新的Pod替换旧的Pod的策略， 支持两个属性：
  type：指定策略类型，支持两种策略
	Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod
	RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod
  rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性：
	maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。
	maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。
```



###### 副本保障机制

controller实时检测pod状态，并保障副本数一直处于期望的值。

```powershell
## 删除pod，观察pod状态变化
$ kubectl -n demo delete pod myblog-7c96c9f76b-qbbg7

# 观察pod
$ kubectl get pods -o wide

## 设置两个副本, 或者通过kubectl -n demo edit deploy myblog的方式，最好通过修改文件，然后apply的方式，这样yaml文件可以保持同步
$ kubectl -n demo scale deploy myblog --replicas=2  #scale扩展
deployment.extensions/myblog scaled

# 观察pod
$ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE
myblog-7c96c9f76b-qbbg7   1/1     Running   0          11m
myblog-7c96c9f76b-s6brm   1/1     Running   0          55s
mysql-85f4f65f99-w6jkj    1/1     Running   0          11m
```

###### Pod驱逐策略

K8S 有个特色功能叫 pod eviction，它在某些场景下如节点 NotReady，或者资源不足时，把 pod 驱逐至其它节点，这也是出于业务保护的角度去考虑的。

1. Kube-controller-manager: 周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，驱逐该节点上所有 pod。停掉kubelet
   - `pod-eviction-timeout`：NotReady 状态节点超过该时间后，执行驱逐，默认 5 min
2. Kubelet: 周期性检查本节点资源，当资源不足时，按照优先级驱逐部分 pod
   - `memory.available`：节点可用内存
   - `nodefs.available`：节点根盘可用存储空间
   - `nodefs.inodesFree`：节点inodes可用数量
   - `imagefs.available`：镜像存储盘的可用空间
   - `imagefs.inodesFree`：镜像存储盘的inodes可用数量



###### 服务更新

修改dockerfile，重新打tag模拟服务更新。

更新方式：

- 修改yaml文件，使用`kubectl -n demo apply -f deploy-myblog.yaml`来应用更新
- `kubectl -n demo edit deploy myblog`在线更新
- `kubectl set image deploy myblog myblog=172.21.32.6:5000/myblog:v2 --record` 

修改文件测试：

```powershell
$ vi mybolg/blog/template/index.html

$ docker build . -t 172.21.32.6:5000/myblog:v2 -f Dockerfile_optimized
$ docker push 172.21.32.6:5000/myblog:v2
```



###### 更新策略

```yaml
...
spec:
  replicas: 2	#指定Pod副本数
  selector:		#指定Pod的选择器
    matchLabels:
      app: myblog
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25% 
    type: RollingUpdate		#指定更新方式为滚动更新，默认策略，通过get deploy yaml查看
    ...
```

![](http://cdn.gtrinee.top/update.png)

策略控制：

- maxSurge：最大激增数, 指更新过程中, 最多可以比replicas预先设定值多出的pod数量, 可以为固定值或百分比,默认为desired Pods数的25%。计算时向上取整(比如3.4，取4)，更新过程中最多会有replicas + maxSurge个pod
- maxUnavailable： 指更新过程中, 最多有几个pod处于无法服务状态 , 可以为固定值或百分比，默认为desired Pods数的25%。计算时向下取整(比如3.6，取3)

*在Deployment rollout时，需要保证Available(Ready) Pods数不低于 desired pods number - maxUnavailable; 保证所有的非异常状态Pods数不多于 desired pods number + maxSurge*。

以myblog为例，使用默认的策略，更新过程:

1. maxSurge 25%，2个实例，向上取整，则maxSurge为1，意味着最多可以有2+1=3个Pod，那么此时会新创建1个ReplicaSet，RS-new，把副本数置为1，此时呢，副本控制器就去创建这个新的Pod
2. 同时，maxUnavailable是25%，副本数2*25%，向下取整，则为0，意味着，滚动更新的过程中，不能有少于2个可用的Pod，因此，旧的Replica（RS-old）会先保持不动，等RS-new管理的Pod状态Ready后，此时已经有3个Ready状态的Pod了，那么由于只要保证有2个可用的Pod即可，因此，RS-old的副本数会有2个变成1个，此时，会删掉一个旧的Pod
3. 删掉旧的Pod的时候，由于总的Pod数量又变成2个了，因此，距离最大的3个还有1个Pod可以创建，所以，RS-new把管理的副本数由1改成2，此时又会创建1个新的Pod，等RS-new管理了2个Pod都ready后，那么就可以把RS-old的副本数由1置为0了，这样就完成了滚动更新



```powershell
#查看滚动更新事件
$ kubectl -n demo describe deploy myblog
...
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  11s   deployment-controller  Scaled up replica set myblog-6cf56fc848 to 1
  Normal  ScalingReplicaSet  11s   deployment-controller  Scaled down replica set myblog-6fdcf98f9 to 1
  Normal  ScalingReplicaSet  11s   deployment-controller  Scaled up replica set myblog-6cf56fc848 to 2
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled down replica set myblog-6fdcf98f9 to 0
$ kubectl get rs
NAME                     DESIRED   CURRENT   READY   AGE
myblog-6cf56fc848   2         2         2       16h
myblog-6fdcf98f9    0         0         0       16h
```

###### 服务回滚

通过滚动升级的策略可以平滑的升级Deployment，若升级出现问题，需要最快且最好的方式回退到上一次能够提供正常工作的版本。为此K8S提供了回滚机制。

**revision**：更新应用时，K8S都会记录当前的版本号，即为revision，当升级出现问题时，可通过回滚到某个特定的revision，默认配置下，K8S只会保留最近的几个revision，可以通过Deployment配置文件中的spec.revisionHistoryLimit属性增加revision数量，默认是10。

查看当前：

```powershell
$ kubectl -n demo rollout history deploy myblog ##CHANGE-CAUSE为空
$ kubectl delete -f deploy-myblog.yaml    ## 方便演示到具体效果，删掉已有deployment
```

记录回滚：

```powershell
$ kubectl create -f deploy-myblog.yaml --record

$ kubectl -n demo set image deploy myblog myblog=172.21.32.6:5000/myblog:v2 --record=true
```

查看deployment更 新历史：

```powershell
$ kubectl -n demo rollout history deploy myblog
deployment.extensions/myblog
REVISION  CHANGE-CAUSE
1         kubectl create --filename=deploy-myblog.yaml --record=true
2         kubectl set image deploy myblog myblog=172.21.32.6:5000/demo/myblog:v1 --record=true
```

回滚到具体的REVISION:

```powershell
$ kubectl -n demo rollout undo deploy myblog --to-revision=1
deployment.extensions/myblog rolled back

# 访问应用测试
```

# Service

通过以前的学习，我们已经能够通过Deployment来创建一组Pod来提供具有高可用性的服务。虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两个问题：

- Pod IP仅仅是集群内可见的虚拟IP，外部无法访问。
- Pod IP会随着Pod的销毁而消失，当ReplicaSet对Pod进行动态伸缩时，Pod IP可能随时随地都会变化，这样对于我们访问这个服务带来了难度。

-

在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不 是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。

 为了解决这个问题，kubernetes提供了Service资源，Service会对**提供同一个服务的多个pod进行聚 合**，并且**提供一个统一的入口地址。**通过访问Service的入口地址就能访问到后面的pod服务。

![image-20230310123135284](http://cdn.gtrinee.top/image-20230310123135284.png)

Service在很多情况下只是一个概念，真正起作用的其实是**kube-proxy服务进程**，每个Node节点上都运 行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信 息，而kube-proxy会基于监听的机制发现这种Service的变动，**然后它会将最新的Service信息转换成对 应的访问规则。**

![image-20230310123458705](http://cdn.gtrinee.top/image-20230310123458705.png)

###### kube-proxy

 运行在每个节点上，监听 API Server 中服务对象的变化，再通过创建流量路由规则来实现网络的转发。[参照]( https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies )

有三种模式：

- User space, 让 Kube-Proxy 在用户空间监听一个端口，所有的 Service 都转发到这个端口，然后 Kube-Proxy 在内部应用层对其进行转发 ， 所有报文都走一遍用户态，性能不高，k8s v1.2版本后废弃。
- Iptables， 当前默认模式，完全由 IPtables 来实现， 通过各个node节点上的iptables规则来实现service的负载均衡，但是随着service数量的增大，iptables模式由于线性查找匹配、全量更新等特点，其性能会显著下降。 
- IPVS， 与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 k8s 1.8版本开始引入，1.11版本开始稳定，需要开启宿主机的ipvs模块。

 IPtables模式示意图：

![](http://cdn.gtrinee.top/services-iptables-overview.svg)









## Ingress

对于Kubernetes的Service，无论是Cluster-Ip和NodePort均是四层的负载，集群内的服务如何实现七层的负载均衡，这就需要借助于Ingress，Ingress控制器的实现方式有很多，比如nginx, Contour, Haproxy, trafik, Istio，我们以nginx的实现为例做演示。

Ingress-nginx是7层的负载均衡器 ，负责统一管理外部对k8s cluster中service的请求。主要包含：

- ingress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件），动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）；
- ingress资源对象：将Nginx的配置抽象成一个Ingress对象，每添加一个新的Service资源对象只需写一个新的Ingress规则的yaml文件即可（或修改已存在的ingress规则的yaml文件）

Service对集群之外**暴露服务的主要方式有两种：NotePort和LoadBalancer**， 但是这两种方式，都有一定的缺点： 

- NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈 发明显 
- LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持 

基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以 满足暴露多个Service的需求。工作机制大致如下图表示：

###### 示意图：

![](http://cdn.gtrinee.top/ingress.webp)

###### 实现逻辑

1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化
2）然后读取ingress规则(规则就是写明了哪个域名对应哪个service)，按照自定义的规则，生成一段nginx配置
3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器把生成的nginx配置写入/etc/nginx.conf文件中
4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。

![image-20230310135007390](http://cdn.gtrinee.top/image-20230310135007390.png)

实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类 似于Nginx，可以理解成在Ingress里建立诸多映射规则，**Ingress Controller通过监听这些配置规则并 转化成Nginx的反向代理配置 , 然后对外部提供服务。**在这里有两个核心概念：





###### Service 负载均衡之Cluster IP

service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP 。使用Service对象，通过selector进行标签选择，找到对应的Pod:

`myblog/deployment/svc-myblog.yaml` 





## endpoint

endpoints是由service定义中的label selector生成的资源对象，它被会存储在k8s的etcd中，代表一个service对应的所有pod副本的访问地址。当请求到达service时，k8s会根据策略从endpoints中的访问地址中选择一个pod并进行访问和操作。当对应地址发生变动时，k8s会基于监听机制发现变动，并根据变动对endpoints进行更新以便于endpoints资源对象实时代表着从service到pod的访问地址。

然而，当service中没有定义label selector字段时，service就无法生成endpoints了。那么此service就无法代理对应的pod了吗？

答案当然是否定的，具体参考官网[Service](https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/concepts/services-networking/service/) 这里做出简单介绍。

endpoints的官网介绍[Endpoints](https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/reference/kubernetes-api/service-resources/endpoints-v1/)



## 2. endpoints资源对象

在k8s集群中，service基于定义的label selector从k8s中选择一系列pod作为提供服务的后端。当service不定义selector时，可以结合endpoints资源对象将使用其他多种类型服务作为后端服务，包括部署在k8s集群外部的服务。例如：

- 在生产环境中，需要访问和操作部署在k8s集群外部的数据库，但是在测试环境中，你只需要使用自己定义的数据库即可。
- 想要将service指向不同namespace甚至是不同namespace中的service
- 在迁移一个负载进入到k8s集群中。在迁移过程中，你可以首先将后端服务的一部分移入k8s集群中

此时， service是没有selector的，举例如下：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

由于service没有定义selector，那么k8s就不会为此service生成对应的endpoints资源对象，就需要通过手动添加endpoints资源对象的方式来指定进入service的请求匹配到的服务地址。

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  # the name here should match the name of the Service
  name: my-service
subsets:
  - addresses:
      - ip: 192.0.2.42
    ports:
      - port: 9376
```



那么endpoints是如何与service建立对应的关系呢？就是name和namespace。endpoints和service应该在同一个namespace中，并且name相同，这样，k8s就可以建立两者之间的联系。

因此：endpoint与对应service的namespace和name必须相同。

在上述例子上，进入service中80端口的请求就会被转发到192.0.2.42：9376端口上。

端口解释：在service中，targetPort就是pod端口，也就是后端所暴露出来的端口。在这里，service没有pod，所以targetPort就是endpoints资源对象中暴露出来的端口号，也就是endpoints资源对象中ports字段定义的端口号。endpoints资源的ports字段用于定义暴露出来的端口，而targetPort就是指这些暴露出来的端口，用于实现service中的port到endpoints端口的映射。

## Service类型

Service的资源清单文件：

```yaml
kind: Service # 资源类型
apiVersion: v1 # 资源版本
metadata: # 元数据
  name: service # 资源名称
  namespace: dev # 命名空间
spec: # 描述
  selector: # 标签选择器，用于确定当前service代理哪些pod
	app: nginx
  type: # Service类型，指定service的访问方式
  clusterIP: # 虚拟服务的ip地址
  sessionAffinity: # session亲和性，支持ClientIP、None两个选项
  ports: # 端口信息
	- protocol: TCP
	  port: 3017 # service端口
	  targetPort: 5003 # pod端口
	  nodePort: 31122 # 主机端口

```

- ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问 
- NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访 问服务 
- LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持 
- ExternalName： 把集群外部的服务引入集群内部，直接使用

myblog/deployment/svc-myblog.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myblog
  namespace: demo
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8002
  selector:
    app: myblog
  type: ClusterIP
```

操作演示：

```powershell
## 别名
$ alias kd='kubectl -n demo'

## 创建服务
$ kd create -f svc-myblog.yaml
$ kd get po --show-labels
NAME                      READY   STATUS    RESTARTS   AGE    LABELS
myblog-5c97d79cdb-jn7km   1/1     Running   0          6m5s   app=myblog
mysql-85f4f65f99-w6jkj    1/1     Running   0          176m   app=mysql

$ kd get svc
NAME     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
myblog   ClusterIP   10.99.174.93   <none>        80/TCP    7m50s

$ kd describe svc myblog
Name:              myblog
Namespace:         demo
Labels:            <none>
Annotations:       <none>
Selector:          app=myblog
Type:              ClusterIP
IP:                10.99.174.93
Port:              <unset>  80/TCP
TargetPort:        8002/TCP
Endpoints:         10.244.0.68:8002
Session Affinity:  None
Events:            <none>

## 扩容myblog服务
$ kd scale deploy myblog --replicas=2
deployment.extensions/myblog scaled

## 再次查看
$ kd describe svc myblog
Name:              myblog
Namespace:         demo
Labels:            <none>
Annotations:       <none>
Selector:          app=myblog
Type:              ClusterIP
IP:                10.99.174.93
Port:              <unset>  80/TCP
TargetPort:        8002/TCP
Endpoints:         10.244.0.68:8002,10.244.1.158:8002
Session Affinity:  None
Events:            <none>
```

Service与Pod如何关联:

service对象创建的同时，会创建同名的endpoints对象，若服务设置了readinessProbe, 当readinessProbe检测失败时，endpoints列表中会剔除掉对应的pod_ip，这样流量就不会分发到健康检测失败的Pod中

```powershell
$ kd get endpoints myblog
NAME     ENDPOINTS                            AGE
myblog   10.244.0.68:8002,10.244.1.158:8002   7m
```

Service Cluster-IP如何访问:

```powershell
$ kd get svc myblog
NAME   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
myblog   ClusterIP   10.99.174.93   <none>        80/TCP    13m
$ curl 10.99.174.93/blog/index/
```

为mysql服务创建service：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: demo
spec:
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: mysql
  type: ClusterIP
```

访问mysql：

```powershell
$ kd get svc mysql
mysql    ClusterIP   10.108.214.84   <none>        3306/TCP   3s
$ curl 10.108.214.84:3306
```

目前使用hostNetwork部署，通过宿主机ip+port访问，弊端：

- 服务使用hostNetwork，使得宿主机的端口大量暴漏，存在安全隐患
- 容易引发端口冲突

服务均属于k8s集群，尽可能使用k8s的网络访问，因此可以对目前myblog访问mysql的方式做改造：

- 为mysql创建一个固定clusterIp的Service，把clusterIp配置在myblog的环境变量中
- 利用集群服务发现的能力，组件之间通过service name来访问

###### 服务发现

在k8s集群中，组件之间可以通过定义的Service名称实现通信。

演示服务发现：

```powershell
## 演示思路：在myblog的容器中直接通过service名称访问服务，观察是否可以访问通

# 先查看服务
$ kd get svc
NAME     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
myblog   ClusterIP   10.99.174.93    <none>        80/TCP     59m
mysql    ClusterIP   10.108.214.84   <none>        3306/TCP   35m

# 进入myblog容器
$ kd exec -ti myblog-5c97d79cdb-j485f bash
[root@myblog-5c97d79cdb-j485f myblog]# curl mysql:3306
5.7.29 )→  (mysql_native_password ot packets out of order
[root@myblog-5c97d79cdb-j485f myblog]# curl myblog/blog/index/
我的博客列表

```

虽然podip和clusterip都不固定，但是service name是固定的，而且具有完全的跨集群可移植性，因此组件之间调用的同时，完全可以通过service name去通信，这样避免了大量的ip维护成本，使得服务的yaml模板更加简单。因此可以对mysql和myblog的部署进行优化改造：

1. mysql可以去掉hostNetwork部署，使得服务只暴漏在k8s集群内部网络
2. configMap中数据库地址可以换成Service名称，这样跨环境的时候，配置内容基本上可以保持不用变化

修改deploy-mysql.yaml

```yaml
    spec:
      hostNetwork: true	# 去掉此行
      volumes: 
      - name: mysql-data
        hostPath: 
          path: /opt/mysql/data

```

修改configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myblog
  namespace: demo
data:
  MYSQL_HOST: "mysql"	# 此处替换为mysql
  MYSQL_PORT: "3306"
```

应用修改：

```powershell
$ kubectl apply -f configmap.yaml
$ kubectl apply -f deploy-mysql.yaml

## 重建pod
$ kubectl -n demo delete po mysql-7f747644b8-6npzn

#去掉taint
$ kubectl taint node k8s-slave1 smoke-
$ kubectl taint node k8s-slave2 drunk-

## myblog不用动，会自动因健康检测不过而重启
```

服务发现实现：

 `CoreDNS`是一个`Go`语言实现的链式插件`DNS服务端`，是CNCF成员，是一个高性能、易扩展的`DNS服务端`。 

```powershell
$ kubectl -n kube-system get po -o wide|grep dns
coredns-d4475785-2w4hk             1/1     Running   0          4d22h   10.244.0.64       
coredns-d4475785-s49hq             1/1     Running   0          4d22h   10.244.0.65

# 查看myblog的pod解析配置
$ kubectl -n demo exec -ti myblog-5c97d79cdb-j485f bash
[root@myblog-5c97d79cdb-j485f myblog]# cat /etc/resolv.conf
nameserver 10.96.0.10
search demo.svc.cluster.local svc.cluster.local cluster.local
options ndots:5

## 10.96.0.10 从哪来
$ kubectl -n kube-system get svc
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP   51d

## 启动pod的时候，会把kube-dns服务的cluster-ip地址注入到pod的resolve解析配置中，同时添加对应的namespace的search域。 因此跨namespace通过service name访问的话，需要添加对应的namespace名称，
service_name.namespace_name
$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   26h
```

###### Service负载均衡之NodePort

cluster-ip为虚拟地址，只能在k8s集群内部进行访问，集群外部如果访问内部服务，实现方式之一为使用NodePort方式。NodePort会默认在 30000-32767 ，不指定的会随机使用其中一个。

`myblog/deployment/svc-myblog-nodeport.yaml`

```powershell
apiVersion: v1
kind: Service
metadata:
  name: myblog-np
  namespace: demo
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8002
  selector:
    app: myblog
  type: NodePort
```

查看并访问服务：

```powershell
$ kd create -f svc-myblog-nodeport.yaml
service/myblog-np created
$ kd get svc
NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
myblog      ClusterIP   10.99.174.93     <none>        80/TCP         102m
myblog-np   NodePort    10.105.228.101   <none>        80:30647/TCP   4s
mysql       ClusterIP   10.108.214.84    <none>        3306/TCP       77m

#集群内每个节点的NodePort端口都会进行监听
$ curl 192.168.136.128:30647/blog/index/
我的博客列表
$ curl 192.168.136.131:30647/blog/index/
我的博客列表
## 浏览器访问
```

思考：

1. NodePort的端口监听如何转发到对应的Pod服务？

2. CLUSTER-IP为虚拟IP，集群内如何通过虚拟IP访问到具体的Pod服务？





# Kubernetes网络模型

目前最流行的CNI插件：

- Flannel
- Calico
- Weave

## [#](https://kuboard.cn/learning/k8s-intermediate/service/cni.html#介绍)介绍

网络架构是Kubernetes中较为复杂、让很多用户头疼的方面之一。Kubernetes网络模型本身对某些特定的网络功能有一定要求，但在实现方面也具有一定的灵活性。因此，业界已有不少不同的网络方案，来满足特定的环境和要求。

CNI意为容器网络接口，它是一种标准的设计，为了让用户在容器创建或销毁时都能够更容易地配置容器网络。在本文中，我们将集中探索与对比目前最流行的CNI插件：Flannel、Calico、Weave和Canal（技术上是多个插件的组合）。这些插件既可以确保满足Kubernetes的网络要求，又能为Kubernetes集群管理员提供他们所需的某些特定的网络功能。

## [#](https://kuboard.cn/learning/k8s-intermediate/service/cni.html#背景)背景

容器网络是容器选择连接到其他容器、主机和外部网络（如Internet）的机制。容器的Runtime提供了各种网络模式，每种模式都会产生不同的体验。

例如，Docker默认情况下可以为容器配置以下网络：

- none：将容器添加到一个容器专门的网络堆栈中，没有对外连接。
- host：将容器添加到主机的网络堆栈中，没有隔离。
- default bridge：默认网络模式。每个容器可以通过IP地址相互连接。
- 自定义网桥：用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。

Docker还可以让用户通过其他驱动程序和插件，来配置更高级的网络（包括多主机覆盖网络）。

CNI的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。下面链接中的CNI规范概括了用于配制网络的插件接口，这个接口可以让容器运行时与插件进行协调：

[CND SPEC(opens new window)](https://github.com/containernetworking/cni/blob/master/SPEC.md)

插件负责为接口配置和管理IP地址，并且通常提供与IP管理、每个容器的IP分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配IP地址并配置网络，并在删除容器时再次调用它以清理这些资源。

运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个veth对的一侧。接着，它会在主机上进行更改，包括将veth的其他部分连接到网桥。再之后，它会通过调用单独的IPAM（IP地址管理）插件来分配IP地址并设置路由。

在Kubernetes中，kubelet可以在适当的时间调用它找到的插件，来为通过kubelet启动的pod进行自动的网络配置。

## [#](https://kuboard.cn/learning/k8s-intermediate/service/cni.html#术语)术语

在对CNI插件们进行比较之前，我们可以先对网络中会见到的相关术语做一个整体的了解。不论是阅读本文，还是今后接触到其他和CNI有关的内容，了解一些常见术语总是非常有用的。

一些最常见的术语包括：

- **第2层网络**：OSI（Open Systems Interconnections，开放系统互连）网络模型的“数据链路”层。第2层网络会处理网络上两个相邻节点之间的帧传递。第2层网络的一个值得注意的示例是以太网，其中MAC表示为子层。
- **第3层网络**：OSI网络模型的“网络”层。第3层网络的主要关注点，是在第2层连接之上的主机之间路由数据包。IPv4、IPv6和ICMP是第3层网络协议的示例。
- **VXLAN**：代表“虚拟可扩展LAN”。首先，VXLAN用于通过在UDP数据报中封装第2层以太网帧来帮助实现大型云部署。VXLAN虚拟化与VLAN类似，但提供更大的灵活性和功能（VLAN仅限于4096个网络ID）。VXLAN是一种封装和覆盖协议，可在现有网络上运行。
- **Overlay网络**：Overlay网络是建立在现有网络之上的虚拟逻辑网络。Overlay网络通常用于在现有网络之上提供有用的抽象，并分离和保护不同的逻辑网络。
- **封装**：封装是指在附加层中封装网络数据包以提供其他上下文和信息的过程。在overlay网络中，封装被用于从虚拟网络转换到底层地址空间，从而能路由到不同的位置（数据包可以被解封装，并继续到其目的地）。
- **网状网络**：网状网络（Mesh network）是指每个节点连接到许多其他节点以协作路由、并实现更大连接的网络。网状网络允许通过多个路径进行路由，从而提供更可靠的网络。网状网格的缺点是每个附加节点都会增加大量开销。
- **BGP**：代表“边界网关协议”，用于管理边缘路由器之间数据包的路由方式。BGP通过考虑可用路径，路由规则和特定网络策略，帮助弄清楚如何将数据包从一个网络发送到另一个网络。BGP有时被用作CNI插件中的路由机制，而不是封装的覆盖网络。

了解了技术术语和支持各类插件的各种技术之后，下面我们可以开始探索一些最流行的CNI插件了。

## VXLAN

[(17条消息) VXLAN详解及实践_vxlan实践_静⋛翕的博客-CSDN博客](https://blog.csdn.net/WuYuChen20/article/details/104515608)

[VXLAN 基础教程：VXLAN 协议原理介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/130277008)





#  Pod 如何接入网络

关于 Pod 如何接入网络这件事情，Kubernetes 做出了明确的选择。具体来说，Kubernetes 要求所有的网络插件实现必须满足如下要求：

- 所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射（network address translation）
- 所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射
- Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个

在这些限制条件下，需要解决如下四种完全不同的网络使用场景的问题：

1. Container-to-Container 的网络
2. Pod-to-Pod 的网络
3. Pod-to-Service 的网络
4. Internet-to-Service 的网络

## Container-to-Container的网络

通常，我们认为虚拟机中的网络通信是直接使用以太网设备进行的，如下图所示：

![K8S教程_Kubernetes网络模型_虚拟机的以太网设备](https://kuboard.cn/assets/img/eth0.c065dbce.png)

实际情况比这个示意图更加复杂一些。Linux系统中，每一个进程都在一个 [network namespace (opens new window)](http://man7.org/linux/man-pages/man8/ip-netns.8.html)中进行通信，network namespace 提供了一个逻辑上的网络堆栈（包含自己的路由、防火墙规则、网络设备）。换句话说，network namespace 为其中的所有进程提供了一个全新的网络堆栈。

Linux 用户可以使用 `ip` 命令创建 network namespace。例如，下面的命令创建了一个新的 network namespace 名称为 `ns1`：

```sh
$ ip netns add ns1
   
```



当创建 network namespace 时，同时将在 `/var/run/netns` 下创建一个挂载点（mount point）用于存储该 namespace 的信息。

执行 `ls /var/run/netns` 命令，或执行 `ip` 命令，可以查看所有的 network namespace：

```sh
$ ls /var/run/netns
ns1
$ ip netns
ns1
 

```



默认情况下，Linux 将所有的进程都分配到 root network namespace，以使得进程可以访问外部网络，如下图所示：

![K8S教程_Kubernetes网络模型_root_network_namespace](https://kuboard.cn/assets/img/root-namespace.bc75d9ba.png)

在 Kubernetes 中，Pod 是一组 docker 容器的集合，这一组 docker 容器将共享一个 network namespace。Pod 中所有的容器都：

- 使用该 network namespace 提供的同一个 IP 地址以及同一个端口空间
- 可以通过 localhost 直接与同一个 Pod 中的另一个容器通信

Kubernetes 为每一个 Pod 都创建了一个 network namespace。具体做法是，把一个 Docker 容器当做 “Pod Container” 用来获取 network namespace，在创建 Pod 中新的容器时，都使用 docker run 的 `--network:container` 功能来加入该 network namespace，参考 [docker run reference (opens new window)](https://docs.docker.com/engine/reference/run/#network-settings)。如下图所示，每一个 Pod 都包含了多个 docker 容器（`ctr*`），这些容器都在同一个共享的 network namespace 中：

![K8S教程_Kubernetes网络模型_pod_network_namespace](https://kuboard.cn/assets/img/pod-namespace.5098bb9c.png)

此外，Pod 中可以定义数据卷，Pod 中的容器都可以共享这些数据卷，并通过挂载点挂载到容器内部不同的路径，具体请参考 [存储卷](https://kuboard.cn/learning/k8s-intermediate/persistent/pv.html)

## [#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#pod-to-pod的网络)Pod-to-Pod的网络

在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。本章节可以帮助我们理解 Kubernetes 是如何在 Pod-to-Pod 通信中使用真实 IP 的，不管两个 Pod 是在同一个节点上，还是集群中的不同节点上。我们将首先讨论通信中的两个 Pod 在同一个节点上的情况，以避免引入跨节点网络的复杂性。

从 Pod 的视角来看，Pod 是在其自身所在的 network namespace 与同节点上另外一个 network namespace 进程通信。在Linux上，不同的 network namespace 可以通过 [Virtual Ethernet Device (opens new window)](http://man7.org/linux/man-pages/man4/veth.4.html)或 ***veth pair*** (两块跨多个名称空间的虚拟网卡)进行通信。为连接 pod 的 network namespace，可以将 ***veth pair*** 的一段指定到 root network namespace，另一端指定到 Pod 的 network namespace。每一组 ***veth pair*** 类似于一条网线，连接两端，并可以使流量通过。节点上有多少个 Pod，就会设置多少组 ***veth pair***。下图展示了 veth pair 连接 Pod 到 root namespace 的情况：

![K8S教程_Kubernetes网络模型_veth_pair_per_pod](https://kuboard.cn/assets/img/pod-veth-pairs.242c359f.png)

此时，我们的 Pod 都有了自己的 network namespace，从 Pod 的角度来看，他们都有自己的以太网卡以及 IP 地址，并且都连接到了节点的 root network namespace。为了让 Pod 可以互相通过 root network namespace 通信，我们将使用 network bridge（网桥）。

Linux Ethernet bridge 是一个虚拟的 Layer 2 网络设备，可用来连接两个或多个网段（network segment）。网桥的工作原理是，在源于目标之间维护一个转发表（forwarding table），通过检查通过网桥的数据包的目标地址（destination）和该转发表来决定是否将数据包转发到与网桥相连的另一个网段。桥接代码通过网络中具备唯一性的网卡MAC地址来判断是否桥接或丢弃数据。

网桥实现了 [ARP (opens new window)](https://en.wikipedia.org/wiki/Address_Resolution_Protocol)协议，以发现链路层与 IP 地址绑定的 MAC 地址。当网桥收到数据帧时，网桥将该数据帧广播到所有连接的设备上（除了发送者以外），对该数据帧做出相应的设备被记录到一个查找表中（lookup table）。后续网桥再收到发向同一个 IP 地址的流量时，将使用查找表（lookup table）来找到对应的 MAC 地址，并转发数据包。

![K8S教程_Kubernetes网络模型_network_bridge_网桥_虚拟网卡](https://kuboard.cn/assets/img/pods-connected-by-bridge.8f775095.png)

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递-pod-to-pod-同节点)数据包的传递：Pod-to-Pod，同节点**

在 network namespace 将每一个 Pod 隔离到各自的网络堆栈的情况下，虚拟以太网设备（virtual Ethernet device）将每一个 namespace 连接到 root namespace，网桥将 namespace 又连接到一起，此时，Pod 可以向同一节点上的另一个 Pod 发送网络报文了。下图演示了同节点上，网络报文从一个Pod传递到另一个Pod的情况。

![K8S教程_Kubernetes网络模型_同节点上Pod之间发送数据包](https://kuboard.cn/assets/img/pod-to-pod-same-node.90e4d5a2.gif)

Pod1 发送一个数据包到其自己的默认以太网设备 `eth0`。

1. 对 Pod1 来说，`eth0` 通过虚拟以太网设备（veth0）连接到 root namespace
2. 网桥 `cbr0` 中为 `veth0` 配置了一个网段。一旦数据包到达网桥，网桥使用[ARP (opens new window)](https://en.wikipedia.org/wiki/Address_Resolution_Protocol)协议解析出其正确的目标网段 `veth1`
3. 网桥 `cbr0` 将数据包发送到 `veth1`
4. 数据包到达 `veth1` 时，被直接转发到 Pod2 的 network namespace 中的 `eth0` 网络设备。

在整个数据包传递过程中，每一个 Pod 都只和 `localhost` 上的 `eth0` 通信，且数包被路由到正确的 Pod 上。与开发人员正常使用网络的习惯没有差异。

Kubernetes 的网络模型规定，在跨节点的情况下 Pod 也必须可以通过 IP 地址访问。也就是说，Pod 的 IP 地址必须始终对集群中其他 Pod 可见；且从 Pod 内部和从 Pod 外部来看，Pod 的IP地址都是相同的。接下来我们讨论跨节点情况下，网络数据包如何传递。

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递-pod-to-pod-跨节点)数据包的传递：Pod-to-Pod，跨节点**

在了解了如何在同节点上 Pod 之间传递数据包之后，我们接下来看看如何在跨节点的 Pod 之间传递数据包。Kubernetes 网络模型要求 Pod 的 IP 在整个网络中都可访问，但是并不指定如何实现这一点。实际上，这是所使用网络插件相关的，但是，仍然有一些模式已经被确立了。

通常，集群中每个节点都被分配了一个 CIDR 网段，指定了该节点上的 Pod 可用的 IP 地址段。一旦发送到该 CIDR 网段的流量到达节点，就由节点负责将流量继续转发给对应的 Pod。下图展示了两个节点之间的数据报文传递过程。

![K8S教程_Kubernetes网络模型_跨节点上Pod之间发送数据包](https://kuboard.cn/assets/img/pod-to-pod-different-nodes.4187b249.gif)

图中，目标 Pod（以绿色高亮）与源 Pod（以蓝色高亮）在不同的节点上，数据包传递过程如下：

1. 数据包从 Pod1 的网络设备 `eth0`，该设备通过 `veth0` 连接到 root namespace
2. 数据包到达 root namespace 中的网桥 `cbr0`
3. 网桥上执行 ARP 将会失败，因为与网桥连接的所有设备中，没有与该数据包匹配的 MAC 地址。一旦 ARP 失败，网桥会将数据包发送到默认路由（root namespace 中的 `eth0` 设备）。此时，数据包离开节点进入网络
4. 假设网络可以根据各节点的CIDR网段，将数据包路由到正确的节点
5. 数据包进入目标节点的 root namespace（VM2 上的 `eth0`）后，通过网桥路由到正确的虚拟网络设备（`veth1`）
6. 最终，数据包通过 `veth1` 发送到对应 Pod 的 `eth0`，完成了数据包传递的过程

通常来说，每个节点知道如何将数据包分发到运行在该节点上的 Pod。一旦一个数据包到达目标节点，数据包的传递方式与同节点上不同Pod之间数据包传递的方式就是一样的了。

此处，我们直接跳过了如何配置网络，以使得数据包可以从一个节点路由到匹配的节点。这些是与具体的网络插件实现相关的，如果感兴趣，可以深入查看某一个网络插件的具体实现。例如，AWS上，亚马逊提供了一个 [Container Network Interface(CNI) plugin (opens new window)](https://github.com/aws/amazon-vpc-cni-k8s)使得 Kubernetes 可以在 Amazon VPC 上执行节点到节点的网络通信。

**Container Network Interface(CNI)** plugin 提供了一组通用 API 用来连接容器与外部网络。具体到容器化应用开发者来说，只需要了解在整个集群中，可以通过 Pod 的 IP 地址直接访问 Pod；网络插件是如何做到跨节点的数据包传递这件事情对容器化应用来说是透明的。AWS 的 CNI 插件通过利用 AWS 已有的 VPC、IAM、Security Group 等功能提供了一个满足 Kubernetes 网络模型要求的，且安全可管理的网络环境。

> 在 EC2（AWS 的虚拟机服务） 中，每一个实例都绑定到一个 elastic network interface （ENI）并且 VPC 中所有的 ENI 都是可连通的。默认情况下，每一个 EC2 实例都有一个唯一的 ENI，但是可以随时为 EC2 实例创建多个 ENI。AWS 的 kubernetes CNI plugin 利用了这个特点，并为节点上的每一个 Pod 都创建了一个新的 ENI。由于在 AWS 的基础设施中， VPC 当中的 ENI 已经相互连接了，这就使得每一个 Pod 的 IP 地址天然就可在 VPC 内直接访问。当 CNI 插件安装到集群上是，每一个节点（EC2实例）创建多个 elastic network interface 并且为其申请到 IP 地址，在节点上形成一个 CIDR 网段。当 Pod 被部署时，kubernetes 集群上以 DaemonSet 形式部署的一段程序将接收到该节点上 kubelet 发出的添加 Pod 到 网络的请求。这段程序将从节点的可用 ENI 池中找出一个可用的 IP 地址，并将 ENI 及 IP 地址分配给 Pod，具体做法是按照 [数据包的传递：Pod-to-Pod，同节点](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递：pod-to-pod，同节点) 中描述的方式在 Linux 内核中连接虚拟网络设备和网桥。此时，Pod 可以被集群内任意节点访问了。

## [#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#pod-to-service的网络)Pod-to-Service的网络

我们已经了解了如何在 Pod 的 IP 地址之间传递数据包。然而，Pod 的 IP 地址并非是固定不变的，随着 Pod 的重新调度（例如水平伸缩、应用程序崩溃、节点重启等），Pod 的 IP 地址将会出现又消失。此时，Pod 的客户端无法得知该访问哪一个 IP 地址。Kubernetes 中，Service 的概念用于解决此问题。

一个 Kubernetes Service 管理了一组 Pod 的状态，可以追踪一组 Pod 的 IP 地址的动态变化过程。一个 Service 拥有一个 IP 地址，并且充当了一组 Pod 的 IP 地址的“虚拟 IP 地址”。任何发送到 Service 的 IP 地址的数据包将被负载均衡到该 Service 对应的 Pod 上。在此情况下，Service 关联的 Pod 可以随时间动态变化，客户端只需要知道 Service 的 IP 地址即可（该地址不会发生变化）。

从效果上来说，Kubernetes 自动为 Service 创建和维护了集群内部的分布式负载均衡，可以将发送到 Service IP 地址的数据包分发到 Service 对应的健康的 Pod 上。接下来我们讨论一下这是怎么做到的。

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#netfilter-and-iptables)netfilter and iptables**

Kubernetes 利用 Linux 内建的网络框架 - `netfilter` 来实现负载均衡。Netfilter 是由 Linux 提供的一个框架，可以通过自定义 handler 的方式来实现多种网络相关的操作。Netfilter 提供了许多用于数据包过滤、网络地址转换、端口转换的功能，通过这些功能，自定义的 handler 可以在网络上转发数据包、禁止数据包发送到敏感的地址，等。

`iptables` 是一个 user-space 应用程序，可以提供基于决策表的规则系统，以使用 netfilter 操作或转换数据包。在 Kubernetes 中，kube-proxy 控制器监听 apiserver 中的变化，并配置 iptables 规则。当 Service 或 Pod 发生变化时（例如 Service 被分配了 IP 地址，或者新的 Pod 被关联到 Service），kube-proxy 控制器将更新 iptables 规则，以便将发送到 Service 的数据包正确地路由到其后端 Pod 上。iptables 规则将监听所有发向 Service 的虚拟 IP 的数据包，并将这些数据包转发到该Service 对应的一个随机的可用 Pod 的 IP 地址，同时 iptables 规则将修改数据包的目标 IP 地址（从 Service 的 IP 地址修改为选中的 Pod 的 IP 地址）。当 Pod 被创建或者被终止时，iptables 的规则也被对应的修改。换句话说，iptables 承担了从 Service IP 地址到实际 Pod IP 地址的负载均衡的工作。

在返回数据包的路径上，数据包从目标 Pod 发出，此时，iptables 规则又将数据包的 IP 头从 Pod 的 IP 地址替换为 Service 的 IP 地址。从请求的发起方来看，就好像始终只是在和 Service 的 IP 地址通信一样。

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#ipvs)IPVS**

Kubernetes v1.11 开始，提供了另一个选择用来实现集群内部的负载均衡：[IPVS](https://kuboard.cn/learning/k8s-intermediate/service/service-details.html#ipvs-代理模式)。 IPVS（IP Virtual Server）也是基于 netfilter 构建的，在 Linux 内核中实现了传输层的负载均衡。IPVS 被合并到 LVS（Linux Virtual Server）当中，充当一组服务器的负载均衡器。IPVS 可以转发 TCP / UDP 请求到实际的服务器上，使得一组实际的服务器看起来像是只通过一个单一 IP 地址访问的服务一样。IPVS 的这个特点天然适合与用在 Kubernetes Service 的这个场景下。

当声明一个 Kubernetes Service 时，你可以指定是使用 iptables 还是 IPVS 来提供集群内的负载均衡工鞥呢。IPVS 是转为负载均衡设计的，并且使用更加有效率的数据结构（hash tables），相较于 iptables，可以支持更大数量的网络规模。当创建使用 IPVS 形式的 Service 时，Kubernetes 执行了如下三个操作：

- 在节点上创建一个 dummy IPVS interface
- 将 Service 的 IP 地址绑定到该 dummy IPVS interface
- 为每一个 Service IP 地址创建 IPVS 服务器

将来，IPVS 有可能成为 kubernetes 中默认的集群内负载均衡方式。这个改变将只影响到集群内的负载均衡，本文后续讨论将以 iptables 为例子，所有讨论对 IPVS 是同样适用的。

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递-pod-to-service)数据包的传递：Pod-to-Service**

![K8S教程_Kubernetes网络模型_数据包的传递_Pod-to-Service](https://kuboard.cn/assets/img/pod-to-service.6718b584.gif)

在 Pod 和 Service 之间路由数据包时，数据包的发起和以前一样：

1. 数据包首先通过 Pod 的 `eth0` 网卡发出
2. 数据包经过虚拟网卡 `veth0` 到达网桥 `cbr0`
3. 网桥上的 APR 协议查找不到该 Service，所以数据包被发送到 root namespace 中的默认路由 - `eth0`
4. 此时，在数据包被 `eth0` 接受之前，数据包将通过 iptables 过滤。iptables 使用其规则（由 kube-proxy 根据 Service、Pod 的变化在节点上创建的 iptables 规则）重写数据包的目标地址（从 Service 的 IP 地址修改为某一个具体 Pod 的 IP 地址）
5. 数据包现在的目标地址是 Pod 4，而不是 Service 的虚拟 IP 地址。iptables 使用 Linux 内核的 `conntrack` 工具包来记录具体选择了哪一个 Pod，以便可以将未来的数据包路由到同一个 Pod。简而言之，iptables 直接在节点上完成了集群内负载均衡的功能。数据包后续如何发送到 Pod 上，其路由方式与 [Pod-to-Pod的网络](https://kuboard.cn/learning/k8s-intermediate/service/network.html#Pod-to-Pod的网络) 中的描述相同。

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递-service-to-pod)数据包的传递：Service-to-Pod**

![K8S教程_Kubernetes网络模型_数据包的传递_service-to-pod](https://kuboard.cn/assets/img/service-to-pod.4393f600.gif)

1. 接收到此请求的 Pod 将会发送返回数据包，其中标记源 IP 为接收请求 Pod 自己的 IP，目标 IP 为最初发送对应请求的 Pod 的 IP
2. 当数据包进入节点后，数据包将经过 iptables 的过滤，此时记录在 `conntrack` 中的信息将被用来修改数据包的源地址（从接收请求的 Pod 的 IP 地址修改为 Service 的 IP 地址）
3. 然后，数据包将通过网桥、以及虚拟网卡 `veth0`
4. 最终到达 Pod 的网卡 `eth0`

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#使用dns)使用DNS**

Kubernetes 也可以使用 DNS，以避免将 Service 的 cluster IP 地址硬编码到应用程序当中。Kubernetes DNS 是 Kubernetes 上运行的一个普通的 Service。每一个节点上的 `kubelet` 都使用该 DNS Service 来执行 DNS 名称的解析。集群中每一个 Service（包括 DNS Service 自己）都被分配了一个 DNS 名称。DNS 记录将 DNS 名称解析到 Service 的 ClusterIP 或者 Pod 的 IP 地址。[SRV 记录](https://kuboard.cn/learning/k8s-intermediate/service/dns.html#srv-记录) 用来指定 Service 的已命名端口。

DNS Pod 由三个不同的容器组成：

- `kubedns`：观察 Kubernetes master 上 Service 和 Endpoints 的变化，并维护内存中的 DNS 查找表
- `dnsmasq`：添加 DNS 缓存，以提高性能
- `sidecar`：提供一个健康检查端点，可以检查 `dnsmasq` 和 `kubedns` 的健康状态

DNS Pod 被暴露为 Kubernetes 中的一个 Service，该 Service 及其 ClusterIP 在每一个容器启动时都被传递到容器中（环境变量及 /etc/resolves），因此，每一个容器都可以正确的解析 DNS。DNS 条目最终由 `kubedns` 解析，`kubedns` 将 DNS 的所有信息都维护在内存中。`etcd` 中存储了集群的所有状态，`kubedns` 在必要的时候将 `etcd` 中的 key-value 信息转化为 DNS 条目信息，以重建内存中的 DNS 查找表。

CoreDNS 的工作方式与 `kubedns` 类似，但是通过插件化的架构构建，因而灵活性更强。自 Kubernetes v1.11 开始，CoreDNS 是 Kubernetes 中默认的 DNS 实现。

## [#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#internet-to-service的网络)Internet-to-Service的网络

前面我们已经了解了 Kubernetes 集群内部的网络路由。下面，我们来探讨一下如何将 Service 暴露到集群外部：

- 从集群内部访问互联网
- 从互联网访问集群内部

[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#出方向-从集群内部访问互联网)出方向 - 从集群内部访问互联网

将网络流量从集群内的一个节点路由到公共网络是与具体网络以及实际网络配置紧密相关的。为了更加具体地讨论此问题，本文将使用 AWS VPC 来讨论其中的具体问题。

在 AWS，Kubernetes 集群在 VPC 内运行，在此处，每一个节点都被分配了一个内网地址（private IP address）可以从 Kubernetes 集群内部访问。为了使访问外部网络，通常会在 VPC 中添加互联网网关（Internet Gateway），以实现如下两个目的：

- 作为 VPC 路由表中访问外网的目标地址
- 提供网络地址转换（NAT Network Address Translation），将节点的内网地址映射到一个外网地址，以使外网可以访问内网上的节点

在有互联网网关（Internet Gateway）的情况下，虚拟机可以任意访问互联网。但是，存在一个小问题：Pod 有自己的 IP 地址，且该 IP 地址与其所在节点的 IP 地址不一样，并且，互联网网关上的 NAT 地址映射只能够转换节点（虚拟机）的 IP 地址，因为网关不知道每个节点（虚拟机）上运行了哪些 Pod （互联网网关不知道 Pod 的存在）。接下来，我们了解一下 Kubernetes 是如何使用 iptables 解决此问题的。

[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递-node-to-internet)数据包的传递：Node-to-Internet

下图中：

1. 数据包从 Pod 的 network namespace 发出
2. 通过 `veth0` 到达虚拟机的 root network namespace
3. 由于网桥上找不到数据包目标地址对应的网段，数据包将被网桥转发到 root network namespace 的网卡 `eth0`。在数据包到达 `eth0` 之前，iptables 将过滤该数据包。
4. 在此处，数据包的源地址是一个 Pod，如果仍然使用此源地址，互联网网关将拒绝此数据包，因为其 NAT 只能识别与节点（虚拟机）相连的 IP 地址。因此，需要 iptables 执行源地址转换（source NAT），这样子，对互联网网关来说，该数据包就是从节点（虚拟机）发出的，而不是从 Pod 发出的
5. 数据包从节点（虚拟机）发送到互联网网关
6. 互联网网关再次执行源地址转换（source NAT），将数据包的源地址从节点（虚拟机）的内网地址修改为网关的外网地址，最终数据包被发送到互联网

在回路径上，数据包沿着相同的路径反向传递，源地址转换（source NAT）在对应的层级上被逆向执行。

![K8S教程_Kubernetes网络模型_数据包的传递_pod-to-internet](https://kuboard.cn/assets/img/pod-to-internet.986cf745.gif)

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#入方向-从互联网访问kubernetes)入方向 - 从互联网访问Kubernetes**

入方向访问（从互联网访问Kubernetes集群）是一个非常棘手的问题。该问题同样跟具体的网络紧密相关，通常来说，入方向访问在不同的网络堆栈上有两个解决方案：

1. Service LoadBalancer
2. Ingress Controller

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#layer-4-loadbalancer)Layer 4：LoadBalancer**

当创建 Kubernetes Service 时，可以指定其类型为 [LoadBalancer](https://kuboard.cn/learning/k8s-intermediate/service/service-types.html#loadbalancer)。 LoadBalancer 的实现由 [cloud controller (opens new window)](https://kubernetes.io/docs/concepts/architecture/cloud-controller/)提供，cloud controller 可以调用云供应商 IaaS 层的接口，为 Kubernetes Service 创建负载均衡器（如果您自建 Kubernetes 集群，可以使用 NodePort 类型的 Service，并手动创建负载均衡器）。用户可以将请求发送到负载均衡器来访问 Kubernetes 中的 Service。

在 AWS，负载均衡器可以将网络流量分发到其目标服务器组（即 Kubernetes 集群中的所有节点）。一旦数据包到达节点，Service 的 iptables 规则将确保其被转发到 Service 的一个后端 Pod。

**[#](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递-loadbalancer-to-service)数据包的传递：LoadBalancer-to-Service**

接下来了解一下 Layer 4 的入方向访问具体是如何做到的：

1. Loadbalancer 类型的 Service 创建后，cloud controller 将为其创建一个负载均衡器
2. 负载均衡器只能直接和节点（虚拟机沟通），不知道 Pod 的存在，当数据包从请求方（互联网）到达 LoadBalancer 之后，将被分发到集群的节点上
3. 节点上的 iptables 规则将数据包转发到合适的 Pod 上 （同 [数据包的传递：Service-to-Pod](https://kuboard.cn/learning/k8s-intermediate/service/network.html#数据包的传递：service-to-pod)）

从 Pod 到请求方的相应数据包将包含 Pod 的 IP 地址，但是请求方需要的是负载均衡器的 IP 地址。iptables 和 `conntrack` 被用来重写返回路径上的正确的 IP 地址。

下图描述了一个负载均衡器和三个集群节点：

1. 请求数据包从互联网发送到负载均衡器
2. 负载均衡器将数据包随机分发到其中的一个节点（虚拟机），此处，我们假设数据包被分发到了一个没有对应 Pod 的节点（VM2）上
3. 在 VM2 节点上，kube-proxy 在节点上安装的 iptables 规则会将该数据包的目标地址判定到对应的 Pod 上（集群内负载均衡将生效）
4. iptables 完成 NAT 映射，并将数据包转发到目标 Pod

![K8S教程_Kubernetes网络模型_数据包的传递_internet-to-service](https://kuboard.cn/assets/img/internet-to-service.b2991f5e.gif)



# Helm 是什么？？

[(19条消息) Helm系列（4）-Helm Kubernetes 包管理器_牛牛Blog的博客-CSDN博客](https://blog.csdn.net/yujia_666/article/details/109565459)

Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。

### Helm 解决了什么痛点？

在 Kubernetes中部署一个可以使用的应用，需要涉及到很多的 Kubernetes 资源的共同协作。比如你安装一个 WordPress 博客，用到了一些 Kubernetes (下面全部简称k8s)的一些资源对象，包括 Deployment 用于部署应用、Service 提供服务发现、Secret 配置 WordPress 的用户名和密码，可能还需要 pv 和 pvc 来提供持久化服务。并且 WordPress 数据是存储在mariadb里面的，所以需要 mariadb 启动就绪后才能启动 WordPress。这些 k8s 资源过于分散，不方便进行管理，直接通过 kubectl 来管理一个应用，你会发现这十分蛋疼。
 所以总结以上，我们在 k8s 中部署一个应用，通常面临以下几个问题：

- 如何统一管理、配置和更新这些分散的 k8s 的应用资源文件
- 如何分发和复用一套应用模板
- 如何将应用的一系列资源当做一个软件包管理

### Helm 相关组件及概念

Helm 包含两个组件，分别是 helm 客户端 和 Tiller 服务器：

- **helm** 是一个命令行工具，用于本地开发及管理chart，chart仓库管理等
- **Tiller** 是 Helm 的服务端。Tiller 负责接收 Helm 的请求，与 k8s 的 apiserver 交互，根据chart 来生成一个 release 并管理 release
- **chart** Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源
- **release** 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release
- Repoistory Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包

### Helm 原理

下面两张图描述了 Helm 的几个关键组件 Helm（客户端）、Tiller（服务器）、Repository（Chart 软件仓库）、Chart（软件包）之间的关系以及它们之间如何通信



![img](https:////upload-images.jianshu.io/upload_images/1674772-a759dd3895b01c70.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

helm 组件通信

![img](https:////upload-images.jianshu.io/upload_images/1674772-1ebfdcfe9e2e9b24.png?imageMogr2/auto-orient/strip|imageView2/2/w/712/format/webp)

helm 架构



**创建release**

- helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息
- helm 客户端指定的 chart 结构和 values 信息通过 gRPC 传递给 Tiller
- Tiller 服务端根据 chart 和 values 生成一个 release
- Tiller 将install release请求直接传递给 kube-apiserver

**删除release**

- helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息
- helm 客户端指定的 chart 结构和 values 信息通过 gRPC 传递给 Tiller
- Tiller 服务端根据 chart 和 values 生成一个 release
- Tiller 将delete release请求直接传递给 kube-apiserver

**更新release**

- helm 客户端将需要更新的 chart 的 release 名称 chart 结构和 value 信息传给 Tiller
- Tiller 将收到的信息生成新的 release，并同时更新这个 release 的 history
- Tiller 将新的 release 传递给 kube-apiserver 进行更新

### chart 的基本结构

Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源。Chart中的文件安装特定的目录结构组织, 最简单的chart 目录如下所示：



![img](https:////upload-images.jianshu.io/upload_images/1674772-f8fc35f6d299ac55.png?imageMogr2/auto-orient/strip|imageView2/2/w/512/format/webp)

chart 结构

- charts 目录存放依赖的chart
- Chart.yaml 包含Chart的基本信息，包括chart版本，名称等
- templates 目录下存放应用一系列 k8s 资源的 yaml 模板
- _helpers.tpl 此文件中定义一些可重用的模板片断，此文件中的定义在任何资源定义模板中可用
- NOTES.txt 介绍chart 部署后的帮助信息，如何使用chart等
- values.yaml 包含了必要的值定义（默认值）, 用于存储 templates 目录中模板文件中用到变量的值

### 安装Helm

Helm 提供了几种安装方式，本文提供两种安装方式，想要查看更多安装方式，请阅读 Helm 的[官方文档](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.helm.sh%2Fusing_helm%2F%23installing-helm)：

- 手动安装方式

```ruby
$ 下载 Helm 二进制文件
$ wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz
$ 解压缩
$ tar -zxvf helm-v2.9.1-linux-amd64.tar.gz
$ 复制 helm 二进制 到bin目录下
$cp linux-amd64/helm /usr/local/bin/
```

- 使用官方提供的脚本一键安装

```ruby
$ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh
$ chmod 700 get_helm.sh
$ ./get_helm.sh
```

你还可以通过 Helm 的 [github](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fhelm%2Fhelm%2Freleases) 项目下找到你想要的 Helm 版本的二进制，然后通过手动安装方式一样安装即可

### 安装 Tiller



# 查看etcd数据

[k8s 中使用 etcd 快照恢复集群数据 - sunls - 博客园 (cnblogs.com)](https://www.cnblogs.com/sunls/p/14565893.html)

拷贝etcdctl命令行工具：

```powershell
$ docker exec -ti  etcd_container which etcdctl
$ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl

[root@master myblog]# chmod +x /usr/bin/etcdctl
```

查看所有key值：

```powershell
$  ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get / --prefix --keys-only
```

查看具体的key对应的数据：

```powershell
$ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get /registry/pods/jenkins/sonar-postgres-7fc5d748b6-gtmsb
```

# 基于EFK实现kubernetes集群的日志平台（扩展）   

##### EFK介绍

EFK工作示意

![](F:\BaiduNetdiskDownload\14小时搞定k8s企业级devops实践\课件\DevOps训练营课件最新版(2020-04-11)\images\EFK-architecture.png)



- Elasticsearch

  一个开源的分布式、Restful 风格的搜索和数据分析引擎，它的底层是开源库Apache Lucene。它可以被下面这样准确地形容：

  - 一个分布式的实时文档存储，每个字段可以被索引与搜索；
  - 一个分布式实时分析搜索引擎；
  - 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。

- Fluentd

  一个针对日志的收集、处理、转发系统。通过丰富的插件系统，可以收集来自于各种系统或应用的日志，转化为用户指定的格式后，转发到用户所指定的日志存储系统之中。 

  ![](F:\BaiduNetdiskDownload\14小时搞定k8s企业级devops实践\课件\DevOps训练营课件最新版(2020-04-11)\images\fluentd-architecture.jpg)

  Fluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储、kafka等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下

  1. 首先 Fluentd 从多个日志源获取数据
  2. 结构化并且标记这些数据
  3. 然后根据匹配的标签将数据发送到多个目标服务

- Kibana

  Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。可以通过Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。也可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。

##### 部署es服务

###### 部署分析

1. es生产环境是部署es集群，通常会使用statefulset进行部署，此例由于演示环境资源问题，部署为单点
2. 数据存储挂载主机路径
3. es默认使用elasticsearch用户启动进程，es的数据目录是通过宿主机的路径挂载，因此目录权限被主机的目录权限覆盖，因此可以利用init container容器在es进程启动之前把目录的权限修改掉，注意init container要用特权模式启动





# Kubernetes日志收集

[Kubernetes日志收集（一）——Pod进程日志收集 - 简书 (jianshu.com)](https://www.jianshu.com/p/92a4c11e77ba)

在搭建Kubernetes集群的过程中，集群的日志收集架构是我们不得不考虑的问题。根据Kubernetes[官方文档](https://link.jianshu.com?t=https%3A%2F%2Fkubernetes.io%2Fdocs%2Fconcepts%2Fcluster-administration%2Flogging%2F)中提供的几种日志收集的架构，我们从中得到了很大的启发，并针对公司内部K8S集群的特殊状况，设计和实现了一套基于Fluentd、Elasticseach、Kibana的日志收集平台。我们认为这套架构存在一定的通用性，所以在这里分为几篇文章分享给大家，希望大家在搭建K8S日志收集架构的时候，可以在我的文章中得到一些启发。

Kubernetes的日志需求主要分为以下三个方面：

- Kubernetes本身日志的收集
- Docker或者Pod进程日志收集
- 应用日志收集

这篇文章主要针对的是上述列表中前两种日志的收集。对于第三种应用日志的收集，我们采用的是在应用Pod里面编排一个日志收集的sidecar镜像，将应用的日志文件以数据卷的方式挂载到sidecar镜像中，sidecar镜像监测到日志文件的变化，并推送到Elasticsearch里。

## 节点日志收集架构

Kubernetes官方对于Kubernetes本身日志的收集和Docker或者Pod进程日志收集的架构图如下:



![img](https:////upload-images.jianshu.io/upload_images/11509489-1158a2f652715dc9.png?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

logging-with-node-agent.png



根据这个架构图，我们设计了自己个性化的Kubernetes节点日志收集架构，并且基于官方的fluentd镜像，构建了自己个性化的日志收集镜像。



![img](https:////upload-images.jianshu.io/upload_images/11509489-1ace0d6998f1a896.png?imageMogr2/auto-orient/strip|imageView2/2/w/936/format/webp)

节点日志收集.png



## Kubectl ：--v 接口调试，以及设置日志输出详细程度

[(18条消息) Kubectl ：--v 接口调试，以及设置日志输出详细程度_琦彦的博客-CSDN博客](https://blog.csdn.net/fly910905/article/details/103896683)

![image-20230316214009813](http://cdn.gtrinee.top/image-20230316214009813.png)



# statefulset  sc的详细说明

[(19条消息) 【Kubernetes】k8s的【statefulset】sc的详细说明与创建使用详细代码_k8s sa sc_/*守护她的笑容的博客-CSDN博客](https://blog.csdn.net/cuichongxin/article/details/121403914)
